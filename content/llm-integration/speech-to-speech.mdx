---
title: "Speech-to-Speech Models for Voice AI"
description: "Deep dive into S2S models including OpenAI Realtime API and Gemini Live, native audio understanding, architecture comparisons, and when to use S2S vs pipeline"
category: "llm-integration"
tags: ["speech-to-speech", "s2s", "openai-realtime", "gemini-live", "native-audio", "voice-ai"]
relatedTopics: ["model-selection", "latency-optimization", "voice-architecture"]
---

# Speech-to-Speech Models for Voice AI

Speech-to-Speech (S2S) models represent a fundamental shift from the traditional pipeline approach. Instead of chaining separate ASR, LLM, and TTS components, S2S models process audio end-to-end through a single neural network.

## Traditional Pipeline vs. S2S

### Traditional Pipeline Architecture

```
+----------+    +----------+    +----------+
|  Audio   |--->|  Text    |--->|  Text    |
|   In     | ASR| (trans)  | LLM| (resp)   |
+----------+    +----------+    +----------+
                                     |
+----------+    +----------+         |
|  Audio   |<---|  Text    |<--------+
|   Out    | TTS| (resp)   |
+----------+    +----------+

Latency: ASR (200ms) + LLM (500ms) + TTS (200ms) = 900ms
```

### Speech-to-Speech Architecture

```
+----------+                  +----------+
|  Audio   |------------------|  Audio   |
|   In     |   Single Model   |   Out    |
+----------+                  +----------+

Latency: 300-500ms (single inference)
```

## Architecture Differences

| Aspect | Pipeline | Speech-to-Speech |
|--------|----------|------------------|
| **Latency** | 800-1500ms | 300-600ms |
| **Component Control** | Individual tuning | Black box |
| **Voice Cloning** | Any TTS voice | Limited voices |
| **Transcription** | Always available | May require separate |
| **Cost** | Sum of components | Single pricing |
| **Emotion Preservation** | Lost in text conversion | Maintained |
| **Interruption Handling** | Complex orchestration | Native support |

## OpenAI Realtime API / GPT-4o Audio

The OpenAI Realtime API represents OpenAI's first production speech-to-speech offering. Unlike traditional pipelines, it processes and generates audio directly through a single model and API.

### Key Capabilities

- Native audio understanding (no intermediate transcription)
- Real-time streaming via WebSocket/WebRTC
- Voice Activity Detection (VAD) with interruption handling
- Multiple voice options with emotion preservation
- Function calling support during audio generation

### Connection Methods

| Method | Use Case | Latency | Complexity |
|--------|----------|---------|------------|
| **WebRTC** | Browser-based apps | Lowest | Medium |
| **WebSocket** | Server-side integration | Low | Low |
| **SIP** | VoIP/telephony systems | Low | High |

### Performance

Based on independent benchmarking:
- **Latency**: Feels "snappy-human" in real-world tests
- **Instruction Following**: 18.6 percentage points improvement over previous snapshot
- **Tool Calling**: 12.9 percentage points improvement

### Pricing (as of 2025)

| Component | Cost |
|-----------|------|
| Audio Input | $100 / 1M tokens (equivalent) |
| Audio Output | $200 / 1M tokens (equivalent) |
| Text Input | $5.00 / 1M tokens |
| Text Output | $15.00 / 1M tokens |

### Implementation Example

```python
import asyncio
import websockets
import json
import base64

class OpenAIRealtimeClient:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.ws_url = "wss://api.openai.com/v1/realtime"
        self.model = "gpt-4o-realtime-preview"

    async def connect(self):
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "OpenAI-Beta": "realtime=v1"
        }

        self.ws = await websockets.connect(
            f"{self.ws_url}?model={self.model}",
            extra_headers=headers
        )

        # Configure session
        await self.ws.send(json.dumps({
            "type": "session.update",
            "session": {
                "modalities": ["text", "audio"],
                "instructions": "You are a helpful voice assistant.",
                "voice": "alloy",
                "input_audio_format": "pcm16",
                "output_audio_format": "pcm16",
                "turn_detection": {
                    "type": "server_vad",
                    "threshold": 0.5,
                    "prefix_padding_ms": 300,
                    "silence_duration_ms": 500
                }
            }
        }))

    async def send_audio(self, audio_chunk: bytes):
        """Send audio chunk for processing."""
        await self.ws.send(json.dumps({
            "type": "input_audio_buffer.append",
            "audio": base64.b64encode(audio_chunk).decode()
        }))

    async def receive_responses(self):
        """Receive and yield audio responses."""
        async for message in self.ws:
            event = json.loads(message)

            if event["type"] == "response.audio.delta":
                audio_data = base64.b64decode(event["delta"])
                yield audio_data

            elif event["type"] == "response.audio.done":
                break
```

## Google Gemini 2.0 Live

Gemini 2.0 Live represents Google's approach to real-time multimodal AI. It uses a stateful WebSocket API that supports simultaneous processing of audio, video, and text.

### Key Features

- Native audio input/output (24 languages, 30 HD voices)
- Multimodal input (audio + video + screen sharing)
- Affective dialog (adapts tone to user emotion)
- Barge-in capability (interrupt mid-response)
- Tool use and Google Search integration
- 1M token context window

### Production Architecture

```
+--------------+     +--------------+     +--------------+
|   Frontend   |     |   Backend    |     |   Gemini     |
|   (Browser)  |---->|   (Proxy)    |---->|   Live API   |
|              |<----|              |<----|              |
+--------------+     +--------------+     +--------------+
     WebRTC           WebSocket            WebSocket
```

**Why Proxy Architecture:**
- Security: API keys stay server-side
- Reliability: Handle reconnection logic
- Logging: Capture conversations for analysis
- Rate limiting: Protect against abuse

### Model Variants

| Model | Context | Audio Quality | Use Case |
|-------|---------|---------------|----------|
| Gemini 2.5 Flash Live | 128K | HD (30 voices) | Production voice |
| Gemini 2.0 Flash Live | 1M | Standard | Long conversations |
| Gemini 2.0 Pro Live | 1M | HD | Complex reasoning |

### Integration with WebRTC (via Daily/Pipecat)

```python
from pipecat.pipeline import Pipeline
from pipecat.services.google import GoogleLLMService
from pipecat.transports.daily import DailyTransport

async def create_voice_agent():
    transport = DailyTransport(
        room_url="https://your-domain.daily.co/room",
        token="your-token",
        bot_name="VoiceAgent"
    )

    llm = GoogleLLMService(
        model="gemini-2.0-flash-live",
        api_key="your-api-key"
    )

    pipeline = Pipeline([
        transport.input(),
        llm,
        transport.output()
    ])

    await pipeline.run()
```

## When to Use S2S vs Pipeline

### Choose Speech-to-Speech When:

- **Ultra-low latency is critical** (under 500ms requirement)
- **Emotional nuance matters** (therapy, coaching, sales)
- **Natural interruption handling needed** (casual conversation)
- **Rapid prototyping** (faster time-to-market)
- **Single-vendor preference** (simpler integration)

### Choose Pipeline When:

- **Custom voice required** (brand voice, voice cloning)
- **Transcription required** (compliance, logging)
- **Specialized ASR needed** (medical terminology, accents)
- **Cost optimization at scale** (mix cheaper components)
- **Component-level A/B testing** (optimize each part)
- **Multi-language support** (different TTS per language)

## Hybrid Architectures

Many production systems use hybrid approaches that leverage the best of both:

```python
class HybridVoiceAgent:
    """
    Uses S2S for latency-critical paths,
    falls back to pipeline for complex tasks.
    """

    def __init__(self):
        self.s2s_client = OpenAIRealtimeClient()
        self.pipeline = TraditionalPipeline()
        self.complexity_classifier = ComplexityClassifier()

    async def handle_turn(self, audio: bytes, context: dict):
        # Quick classification on first few hundred ms
        complexity = await self.classify_complexity(audio)

        if complexity == "simple":
            # Use S2S for fast, simple responses
            # Greetings, confirmations, simple queries
            return await self.s2s_client.process(audio)
        else:
            # Use pipeline for complex reasoning
            # Multi-step tasks, tool use, long responses
            transcript = await self.pipeline.transcribe(audio)
            response = await self.pipeline.generate(transcript, context)
            audio_out = await self.pipeline.synthesize(response)
            return audio_out

    async def classify_complexity(self, audio: bytes) -> str:
        """Quick classification of query complexity."""
        # Use lightweight model or heuristics
        # Consider: query length, detected intents, conversation state
        pass
```

## Native Audio Understanding

S2S models understand audio natively, providing capabilities lost in text conversion:

### Preserved Information

| Feature | Pipeline | S2S |
|---------|----------|-----|
| Tone of voice | Lost | Preserved |
| Speaking pace | Lost | Preserved |
| Emotional state | Partially lost | Preserved |
| Hesitation/uncertainty | Lost | Detected |
| Emphasis on words | Lost | Understood |
| Background context | Lost | Can process |

### Use Cases Benefiting from Native Audio

1. **Sales calls** - Detect buyer interest/hesitation
2. **Customer support** - Recognize frustration early
3. **Healthcare** - Voice-based health indicators
4. **Education** - Detect confusion in learners
5. **Accessibility** - Better understanding of atypical speech

## Performance Comparison

| Metric | Traditional Pipeline | OpenAI Realtime | Gemini Live |
|--------|---------------------|-----------------|-------------|
| End-to-end latency | 800-1500ms | 300-600ms | 400-700ms |
| First audio | ~400ms | ~200ms | ~300ms |
| Interruption handling | Complex | Native | Native |
| Voice options | Unlimited | ~6 voices | ~30 voices |
| Transcription | Included | Extra request | Included |
| Max context | Varies | 128K equiv | 1M |

## Cost Considerations

For a typical 5-minute conversation:

| Approach | Estimated Cost |
|----------|---------------|
| Pipeline (budget) | $0.02-0.05 |
| Pipeline (premium) | $0.10-0.20 |
| OpenAI Realtime | $0.50-1.00 |
| Gemini Live | $0.15-0.30 |

S2S models currently cost more but provide superior experience for latency-sensitive applications. As the technology matures, costs are expected to decrease.

## Best Practices

1. **Start with S2S for prototypes** - Faster to build, validate use case
2. **Switch to pipeline for scale** - More cost control, customization
3. **Use hybrid for production** - Best of both worlds
4. **Monitor transcription needs** - S2S may require separate ASR for logs
5. **Test with real users** - Latency perception varies by use case
6. **Plan for fallback** - S2S services can have capacity limits
