---
title: "Voice AI Orchestration Frameworks and Infrastructure"
description: "Guide to orchestration frameworks (LiveKit, Pipecat, Vocode), scalability patterns, and infrastructure decisions for production voice AI"
---

# Voice AI Orchestration Frameworks and Infrastructure

Building production voice AI requires robust orchestration frameworks that handle real-time audio processing, state management, and scaling. This guide examines the leading orchestration frameworks, scalability patterns, and critical infrastructure decisions for deploying voice AI at scale.

## LiveKit Agents

LiveKit Agents is a production-ready, open-source framework for building real-time voice AI applications. It provides a complete solution built on the LiveKit WebRTC infrastructure.

```
+----------------------------------------------------------------------------+
|                       LIVEKIT AGENTS ARCHITECTURE                           |
+----------------------------------------------------------------------------+
|                                                                             |
|  +-----------------------------------------------------------------------+  |
|  |                     LIVEKIT CLOUD / SELF-HOSTED                       |  |
|  |  +-----------+  +-------------+  +-------------+                      |  |
|  |  |   WebRTC  |  |    Room     |  |   Agent     |                      |  |
|  |  |    SFU    |<-|   Server    |->|  Dispatcher |                      |  |
|  |  +-----------+  +-------------+  +------+------+                      |  |
|  +------------------------------------------|-----------------------------|  |
|                                             |                               |
|                                             v                               |
|  +-----------------------------------------------------------------------+  |
|  |                        AGENT WORKER POOL                              |  |
|  |  +------------------------------------------------------------------+ |  |
|  |  |  VoicePipelineAgent                                              | |  |
|  |  |  +-- STT Plugin (Deepgram, AssemblyAI, Whisper)                  | |  |
|  |  |  +-- LLM Plugin (OpenAI, Anthropic, Local)                       | |  |
|  |  |  +-- TTS Plugin (ElevenLabs, Cartesia, XTTS)                     | |  |
|  |  |  +-- VAD (Silero, WebRTC)                                        | |  |
|  |  |  +-- Function Calling / Tools                                    | |  |
|  |  +------------------------------------------------------------------+ |  |
|  +-----------------------------------------------------------------------+  |
|                                                                             |
|  KEY FEATURES:                                                              |
|  - Turn detection with configurable endpointing                             |
|  - Interruption handling (barge-in)                                         |
|  - Multi-agent handoff support                                              |
|  - Built-in load balancing and failover                                     |
|  - Kubernetes-native deployment                                             |
|  - Apache 2.0 open source license                                           |
|                                                                             |
+----------------------------------------------------------------------------+
```

### LiveKit Agent Code Example

```python
from livekit.agents import AutoSubscribe, JobContext, WorkerOptions, cli
from livekit.agents.voice_assistant import VoiceAssistant
from livekit.plugins import deepgram, openai, silero

async def entrypoint(ctx: JobContext):
    # Connect to the room
    await ctx.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)

    # Initialize components
    vad = silero.VAD.load()
    stt = deepgram.STT()
    llm = openai.LLM(model="gpt-4o-mini")
    tts = openai.TTS(voice="alloy")

    # Create voice assistant
    assistant = VoiceAssistant(
        vad=vad,
        stt=stt,
        llm=llm,
        tts=tts,
        # Latency optimization settings
        min_endpointing_delay=0.5,  # 500ms silence before end-of-turn
        interrupt_speech_duration=0.5,  # Allow interruption after 500ms
        preemptive_synthesis=True,  # Begin TTS before LLM completes
    )

    # Start the assistant
    assistant.start(ctx.room)

    # Initial greeting
    await assistant.say("Hello! How can I help you today?")

if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
```

## Pipecat

Pipecat is an open-source Python framework from Daily, focused on flexibility and composability for voice AI pipelines.

```
+----------------------------------------------------------------------------+
|                         PIPECAT ARCHITECTURE                                |
+----------------------------------------------------------------------------+
|                                                                             |
|  PIPELINE CONCEPT:                                                          |
|  +-----------------------------------------------------------------------+  |
|  |                                                                       |  |
|  |  +---------+   +---------+   +---------+   +---------+               |  |
|  |  |Transport|-->|  VAD    |-->|  STT    |-->|Context  |               |  |
|  |  |(Daily,  |   |Processor|   |Processor|   |Aggregator               |  |
|  |  |WebSocket)   +---------+   +---------+   +----+----+               |  |
|  |  +---------+                                    |                     |  |
|  |       ^                                         v                     |  |
|  |       |                                   +---------+                 |  |
|  |       |                                   |   LLM   |                 |  |
|  |       |                                   |Processor|                 |  |
|  |       |                                   +----+----+                 |  |
|  |       |                                        |                      |  |
|  |       |    +---------+   +---------+         |                      |  |
|  |       +----| Audio   |<--|   TTS   |<--------+                      |  |
|  |            | Output  |   |Processor|                                 |  |
|  |            +---------+   +---------+                                 |  |
|  |                                                                       |  |
|  +-----------------------------------------------------------------------+  |
|                                                                             |
|  FRAME-BASED PROCESSING:                                                    |
|  - Data flows as typed frames through pipeline                              |
|  - Processors transform/filter frames                                       |
|  - Supports parallel and sequential processing                              |
|  - Easy to add custom processors                                            |
|                                                                             |
|  KEY DIFFERENTIATORS:                                                       |
|  - Maximum flexibility - no vendor lock-in                                  |
|  - Rich processor ecosystem                                                 |
|  - Excellent for custom/experimental pipelines                              |
|  - Strong Daily.co integration                                              |
|                                                                             |
+----------------------------------------------------------------------------+
```

### Pipecat Code Example

```python
import asyncio
from pipecat.frames.frames import EndFrame, TextFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineTask
from pipecat.processors.aggregators.llm_response import LLMResponseAggregator
from pipecat.processors.aggregators.sentence import SentenceAggregator
from pipecat.services.deepgram import DeepgramSTTService, DeepgramTTSService
from pipecat.services.openai import OpenAILLMService
from pipecat.transports.services.daily import DailyTransport

async def main():
    # Initialize transport
    transport = DailyTransport(
        room_url="https://your-domain.daily.co/room",
        token="your-token",
        bot_name="Voice Assistant"
    )

    # Initialize services
    stt = DeepgramSTTService(api_key="your-key")
    llm = OpenAILLMService(
        api_key="your-key",
        model="gpt-4o-mini"
    )
    tts = DeepgramTTSService(
        api_key="your-key",
        voice="aura-asteria-en"
    )

    # Build pipeline
    pipeline = Pipeline([
        transport.input(),           # Audio from user
        stt,                         # Speech-to-text
        LLMResponseAggregator(),     # Collect full utterances
        llm,                         # Generate response
        SentenceAggregator(),        # Buffer for TTS
        tts,                         # Text-to-speech
        transport.output()           # Audio to user
    ])

    # Run
    runner = PipelineRunner()
    task = PipelineTask(pipeline)
    await runner.run(task)

asyncio.run(main())
```

## Vocode

Vocode provides a hosted platform and SDK for voice AI applications, with strong telephony integration.

```
+----------------------------------------------------------------------------+
|                          VOCODE ARCHITECTURE                                |
+----------------------------------------------------------------------------+
|                                                                             |
|  DEPLOYMENT OPTIONS:                                                        |
|  +-----------------------------------------------------------------------+  |
|  |                                                                       |  |
|  |  HOSTED (Vocode Cloud)         SELF-HOSTED (Open Source)              |  |
|  |  ---------------------         -------------------------               |  |
|  |  - API-driven                  - Full control                          |  |
|  |  - Managed infrastructure      - Docker/Kubernetes                     |  |
|  |  - Quick deployment            - Custom integrations                   |  |
|  |  - Pay-per-minute             - No usage fees                          |  |
|  |                                                                       |  |
|  +-----------------------------------------------------------------------+  |
|                                                                             |
|  CONVERSATION ABSTRACTION:                                                  |
|  +-----------------------------------------------------------------------+  |
|  |                                                                       |  |
|  |  ConversationConfig                                                   |  |
|  |  +-- Transcriber (STT)                                                |  |
|  |  |   +-- DeepgramTranscriber, AssemblyAITranscriber, etc.             |  |
|  |  +-- Agent (LLM)                                                      |  |
|  |  |   +-- ChatGPTAgent, ClaudeAgent, CustomAgent                       |  |
|  |  +-- Synthesizer (TTS)                                                |  |
|  |  |   +-- ElevenLabsSynthesizer, AzureSynthesizer, etc.                |  |
|  |  +-- Telephony (optional)                                             |  |
|  |      +-- TwilioConfig, VonageConfig                                   |  |
|  |                                                                       |  |
|  +-----------------------------------------------------------------------+  |
|                                                                             |
|  USE CASES:                                                                 |
|  - Outbound calling campaigns                                               |
|  - Inbound call handling                                                    |
|  - Voice-enabled chatbots                                                   |
|  - Telephony automation                                                     |
|                                                                             |
+----------------------------------------------------------------------------+
```

## Framework Comparison

| Aspect | LiveKit Agents | Pipecat | Vocode |
|--------|----------------|---------|--------|
| **License** | Apache 2.0 | BSD-2 | MIT |
| **Language** | Python | Python | Python |
| **Transport** | WebRTC (native) | Daily, WebSocket | WebSocket, Telephony |
| **Design Philosophy** | Opinionated, batteries-included | Flexible, composable | Simple, telephony-first |
| **Learning Curve** | Medium | Low-Medium | Low |
| **Customization** | Good | Excellent | Good |
| **Turn Detection** | Built-in | VAD processor | Built-in |
| **Interruption** | Native | Configurable | Configurable |
| **Multi-Agent** | Yes | Manual | Limited |
| **Function Calling** | Yes | Yes | Yes |
| **Telephony** | Via SIP | Via Twilio | Native |
| **Scaling** | K8s HPA | Manual/K8s | Hosted/Manual |
| **Typical Latency** | 400-600ms | 500-800ms | 600-900ms |

### Recommendation by Use Case

| Use Case | Recommended | Rationale |
|----------|-------------|-----------|
| Production voice AI | LiveKit Agents | WebRTC apps, multi-agent systems |
| Custom pipelines | Pipecat | Experimentation, Daily.co integration |
| Telephony automation | Vocode | Quick prototypes, outbound campaigns |

## Custom Orchestration Patterns

For complex requirements, custom orchestration may be necessary.

```python
class VoiceOrchestrator:
    """
    Custom voice AI orchestrator with fine-grained control.
    """

    def __init__(self, config: OrchestratorConfig):
        self.stt = STTService(config.stt)
        self.llm = LLMService(config.llm)
        self.tts = TTSService(config.tts)
        self.vad = VADService(config.vad)

        # State management
        self.state = ConversationState()
        self.audio_buffer = AudioBuffer()
        self.response_queue = asyncio.Queue()

        # Metrics
        self.metrics = LatencyTracker()

    async def process_audio_stream(
        self,
        audio_stream: AsyncIterator[bytes]
    ) -> AsyncIterator[bytes]:
        """Main processing loop with concurrent execution"""

        async def stt_pipeline():
            """Continuous STT processing"""
            async for audio_chunk in audio_stream:
                # Check for interruption during agent speech
                if self.state.is_speaking and self.vad.detect(audio_chunk):
                    await self._handle_interruption()

                # Feed to STT
                transcript = await self.stt.process(audio_chunk)
                if transcript:
                    await self._handle_transcript(transcript)

        async def response_pipeline():
            """Response generation and synthesis"""
            while True:
                transcript = await self.response_queue.get()

                # Generate response with streaming
                async for token in self.llm.stream(
                    self._build_prompt(transcript)
                ):
                    # Stream tokens to TTS
                    if self._is_speakable(token):
                        async for audio in self.tts.stream(token):
                            yield audio

        # Run pipelines concurrently
        async with asyncio.TaskGroup() as tg:
            tg.create_task(stt_pipeline())
            async for audio in response_pipeline():
                yield audio

    async def _handle_interruption(self):
        """Cancel current response and switch to listening"""
        self.state.is_speaking = False
        self.tts.cancel()
        self.llm.cancel()
        self.metrics.record("interruption")

    def _is_speakable(self, buffer: str) -> bool:
        """Determine if buffer is ready for TTS"""
        # Sentence boundary
        if buffer.rstrip().endswith(('.', '!', '?')):
            return True
        # Clause boundary with minimum length
        if len(buffer) > 50 and buffer.rstrip().endswith((',', ';', ':')):
            return True
        return False
```

## Horizontal Scaling

```
+----------------------------------------------------------------------------+
|                    HORIZONTAL SCALING ARCHITECTURE                          |
+----------------------------------------------------------------------------+
|                                                                             |
|  +-----------------------------------------------------------------------+  |
|  |                      LOAD BALANCER (L7)                               |  |
|  |  - WebSocket-aware (sticky sessions)                                  |  |
|  |  - Health checking                                                    |  |
|  |  - Latency-based routing                                              |  |
|  +---------------------------------+-------------------------------------+  |
|                                    |                                        |
|           +------------------------+------------------------+               |
|           v                        v                        v               |
|  +-----------------+  +-----------------+  +-----------------+              |
|  |  Voice Agent   |  |  Voice Agent   |  |  Voice Agent   |              |
|  |  Instance 1    |  |  Instance 2    |  |  Instance N    |              |
|  |  +-----------+ |  |  +-----------+ |  |  +-----------+ |              |
|  |  | 50 active | |  |  | 50 active | |  |  | 50 active | |              |
|  |  |  calls    | |  |  |  calls    | |  |  |  calls    | |              |
|  |  +-----------+ |  |  +-----------+ |  |  +-----------+ |              |
|  +--------+-------+  +--------+-------+  +--------+-------+              |
|           |                   |                   |                        |
|           +-------------------+-------------------+                        |
|                               v                                            |
|  +-----------------------------------------------------------------------+  |
|  |                     SHARED SERVICES                                   |  |
|  |  +---------+  +---------+  +---------+  +---------+                  |  |
|  |  |  Redis  |  | Message |  | State   |  | Metrics |                  |  |
|  |  | (Cache) |  |  Queue  |  |  Store  |  | (Prom)  |                  |  |
|  |  +---------+  +---------+  +---------+  +---------+                  |  |
|  +-----------------------------------------------------------------------+  |
|                                                                             |
+----------------------------------------------------------------------------+
```

### Kubernetes Deployment

```yaml
# Voice AI Agent Kubernetes Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: voice-agent
spec:
  replicas: 3
  selector:
    matchLabels:
      app: voice-agent
  template:
    metadata:
      labels:
        app: voice-agent
    spec:
      containers:
      - name: voice-agent
        image: voice-ai/agent:latest
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
            nvidia.com/gpu: "1"  # For local model inference
          limits:
            memory: "8Gi"
            cpu: "4"
            nvidia.com/gpu: "1"
        env:
        - name: MAX_CONCURRENT_CALLS
          value: "50"
        - name: STT_PROVIDER
          value: "deepgram"
        - name: LLM_PROVIDER
          value: "openai"
        ports:
        - containerPort: 8080
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 3
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: voice-agent-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: voice-agent
  minReplicas: 3
  maxReplicas: 100
  metrics:
  - type: Pods
    pods:
      metric:
        name: active_calls
      target:
        type: AverageValue
        averageValue: "40"  # Scale when avg calls > 40/pod
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

## Infrastructure Decisions

### Cloud vs Edge Deployment

| Factor | Cloud | Edge | Hybrid |
|--------|-------|------|--------|
| **Network RTT** | 50-150ms | 10-30ms | 10-50ms |
| **Total E2E** | 800-1200ms | 350-550ms | 400-700ms |
| **Upfront Cost** | Low | High | Medium |
| **Per-minute Cost** | $0.03-0.10 | $0.01-0.03 | $0.02-0.05 |
| **Break-even** | N/A | ~6-12 months | ~4-8 months |
| **Max Concurrent** | ~Unlimited | Limited by HW | High |
| **Scale Speed** | Seconds | Days/weeks | Minutes/hours |
| **Complexity** | Low | High | Medium |
| **Data Residency** | Provider regions | Full control | Configurable |

### Recommendation by Use Case

| Use Case | Recommended | Rationale |
|----------|-------------|-----------|
| Startup/Prototype | Cloud | Fast iteration, low upfront |
| Customer Service (general) | Cloud/Hybrid | Scale flexibility, reliability |
| Premium Voice Assistant | Edge | Latency critical, quality UX |
| Healthcare/Finance | Hybrid | Compliance + performance |
| Global Enterprise | Hybrid | Regional presence + fallback |
| Call Center (1000+ agents) | Edge | Cost optimization at scale |

### GPU Requirements

| Component | GPU Memory | Compute (TFLOPS) | Typical GPU |
|-----------|------------|------------------|-------------|
| Whisper Large | 3GB | 10 | RTX 3090, L4 |
| Whisper Base | 1GB | 5 | RTX 3070, T4 |
| Llama 8B Q4 | 6GB | 20 | RTX 4090, L4 |
| Llama 8B FP16 | 16GB | 30 | A10G, L40 |
| Llama 70B Q4 | 40GB | 80 | A100, H100 |
| TTS (XTTS) | 2GB | 5 | Any modern GPU |

### Concurrent Capacity per GPU

| GPU | VRAM | LLM Size | Concurrent Calls |
|-----|------|----------|------------------|
| NVIDIA T4 | 16GB | 8B Q4 | 10-20 |
| NVIDIA L4 | 24GB | 8B Q4 | 30-50 |
| NVIDIA A10G | 24GB | 8B FP16 | 20-40 |
| NVIDIA L40 | 48GB | 13B FP16 | 40-60 |
| NVIDIA A100 | 80GB | 70B Q4 | 30-50 |
| NVIDIA H100 | 80GB | 70B FP16 | 50-100 |

## Failover and Disaster Recovery

### Component-Level Failover

```python
async def call_with_fallback(primary, *backups):
    """Execute with automatic failover to backup services"""
    for service in [primary, *backups]:
        try:
            return await asyncio.wait_for(
                service.call(),
                timeout=service.timeout
            )
        except (TimeoutError, ServiceError):
            metrics.record(f"{service.name}_failover")
            continue
    raise AllServicesFailedError()
```

**Failover Chains:**
- **STT**: Deepgram (primary) -> AssemblyAI (backup) -> Whisper Local
- **LLM**: OpenAI GPT-4o (primary) -> Anthropic Claude (backup) -> Local Llama
- **TTS**: ElevenLabs (primary) -> Cartesia (backup) -> Azure TTS

### Region-Level Failover

**Active-Active Configuration:**
- All regions serve traffic simultaneously
- DNS health checks remove unhealthy regions
- Automatic traffic redistribution

**Recovery Objectives:**
- RTO (Recovery Time Objective): under 30 seconds
- RPO (Recovery Point Objective): 0 (no data loss)

### Session Continuity

```python
class SessionManager:
    """Maintain session state across failovers"""

    async def save_state(self, session_id, state):
        # Write to primary and replica atomically
        await self.redis.set(
            f"session:{session_id}",
            state.serialize(),
            ex=3600  # 1 hour TTL
        )

    async def restore_state(self, session_id) -> SessionState:
        data = await self.redis.get(f"session:{session_id}")
        return SessionState.deserialize(data)
```

## Cost Optimization Strategies

### Component-Level Costs (per minute of conversation)

| Component | Cloud API | Self-Hosted | Savings |
|-----------|-----------|-------------|---------|
| STT | $0.006-0.015 | $0.001-0.003 | 70-80% |
| LLM | $0.010-0.050 | $0.002-0.010 | 60-80% |
| TTS | $0.012-0.024 | $0.002-0.005 | 70-85% |
| Telephony | $0.007-0.015 | $0.003-0.008 | 40-50% |
| **TOTAL** | **$0.035-0.104** | **$0.008-0.026** | **65-80%** |

### Optimization Techniques

1. **Model Selection**
   - Use smallest model that meets quality requirements
   - GPT-4o-mini instead of GPT-4o (10x cheaper)
   - Llama 8B instead of 70B for simple tasks

2. **Caching**
   - TTS cache: 20-40% hit rate typical
   - Semantic LLM cache: 5-15% hit rate
   - Savings: 10-30% overall

3. **Batching** (where latency permits)
   - Batch STT requests for async use cases
   - LLM continuous batching (vLLM)
   - Savings: 30-50% compute cost

4. **Spot/Preemptible Instances**
   - Use for non-critical workloads
   - Savings: 60-80% compute cost
   - Requires robust failover

5. **Reserved Capacity**
   - Commit to usage for discounts
   - AWS Reserved Instances: 30-60% savings
   - GCP Committed Use: 30-57% savings

## Summary

Selecting the right orchestration framework and infrastructure is critical for production voice AI:

1. **Framework Selection**: Match framework capabilities to your use case complexity and team expertise
2. **Scaling Strategy**: Plan for horizontal scaling with stateless agents and shared state stores
3. **Infrastructure Mix**: Consider hybrid cloud/edge deployment based on latency and cost requirements
4. **Resilience**: Implement component and region-level failover for high availability
5. **Cost Management**: Optimize through model selection, caching, and appropriate infrastructure choices

The orchestration layer should abstract the complexity of real-time audio processing while providing the hooks necessary for monitoring, debugging, and scaling your voice AI application.
