---
title: "Cost Optimization for Voice AI"
description: "Comprehensive guide to understanding, analyzing, and optimizing costs for voice AI systems including STT, LLM, TTS, and telephony components"
category: Enterprise
tags:
  - cost-optimization
  - economics
  - ROI
  - build-vs-buy
  - infrastructure
related:
  - security-compliance
  - scalability
  - analytics
lastUpdated: "2025-01-21"
difficulty: advanced
---

# Cost Optimization for Voice AI

Voice AI systems involve multiple cost components that can quickly accumulate without careful management. Understanding these costs and implementing optimization strategies is essential for building economically viable voice AI applications. This guide provides a comprehensive framework for analyzing, optimizing, and managing voice AI costs.

## Voice AI Cost Components

A voice AI system incurs costs across four primary categories: Speech-to-Text (STT), Large Language Models (LLM), Text-to-Speech (TTS), and telephony infrastructure.

```
+-----------------------------------------------------------------------------+
|                    VOICE AI COST BREAKDOWN                                    |
+-----------------------------------------------------------------------------+
|                                                                               |
|  TYPICAL COST DISTRIBUTION (per minute of conversation)                       |
|                                                                               |
|  +----------------------------------------------------------------------+    |
|  |                                                                        |    |
|  |  STT (Speech-to-Text)        $0.004 - $0.024/min     [15-25%]        |    |
|  |  +--------------------------+                                         |    |
|  |                                                                        |    |
|  |  LLM (Language Model)        $0.002 - $0.050/min     [20-50%]        |    |
|  |  +--------------------------------+                                   |    |
|  |                                                                        |    |
|  |  TTS (Text-to-Speech)        $0.008 - $0.024/min     [15-25%]        |    |
|  |  +--------------------------+                                         |    |
|  |                                                                        |    |
|  |  Telephony                   $0.005 - $0.020/min     [10-20%]        |    |
|  |  +----------------------+                                             |    |
|  |                                                                        |    |
|  |  Infrastructure              $0.002 - $0.010/min     [5-15%]         |    |
|  |  +----------------+                                                   |    |
|  |                                                                        |    |
|  +----------------------------------------------------------------------+    |
|                                                                               |
|  TOTAL RANGE: $0.02 - $0.15 per minute of conversation                       |
|                                                                               |
+-----------------------------------------------------------------------------+
```

### Detailed Cost Analysis by Component

#### Speech-to-Text (STT) Costs

| Provider | Pricing Model | Cost/Minute | Notes |
|----------|---------------|-------------|-------|
| Google Speech-to-Text | Per 15-second interval | $0.004-$0.009 | Cheaper for short utterances |
| Amazon Transcribe | Per second | $0.0004/sec ($0.024/min) | Volume discounts available |
| Azure Speech | Per audio hour | $0.0167/min | Batch processing cheaper |
| Deepgram | Per minute | $0.0043-$0.0145 | Nova models most cost-effective |
| Whisper (OpenAI) | Per minute | $0.006 | High accuracy |
| Whisper (Self-hosted) | GPU compute | $0.001-$0.004 | Depends on GPU utilization |
| AssemblyAI | Per second | $0.00025/sec | Real-time streaming |

#### Large Language Model (LLM) Costs

| Provider/Model | Input Cost | Output Cost | Typical Cost/Turn |
|----------------|------------|-------------|-------------------|
| GPT-4o | $2.50/1M tokens | $10.00/1M tokens | $0.003-$0.008 |
| GPT-4o-mini | $0.15/1M tokens | $0.60/1M tokens | $0.0002-$0.0005 |
| Claude 3.5 Sonnet | $3.00/1M tokens | $15.00/1M tokens | $0.004-$0.010 |
| Claude 3 Haiku | $0.25/1M tokens | $1.25/1M tokens | $0.0003-$0.0008 |
| Llama 3.1 70B (hosted) | $0.88/1M tokens | $0.88/1M tokens | $0.001-$0.003 |
| Llama 3.1 8B (self-hosted) | GPU compute | GPU compute | $0.0002-$0.001 |
| Gemini 1.5 Flash | $0.075/1M tokens | $0.30/1M tokens | $0.0001-$0.0003 |

#### Text-to-Speech (TTS) Costs

| Provider | Pricing Model | Cost/1K chars | Notes |
|----------|---------------|---------------|-------|
| Google Cloud TTS | Per character | $0.004-$0.016 | WaveNet more expensive |
| Amazon Polly | Per character | $0.004 | Neural voices extra |
| Azure TTS | Per character | $0.004-$0.016 | Custom voices available |
| ElevenLabs | Per character | $0.018-$0.030 | Premium voice quality |
| Play.ht | Per character | $0.015-$0.025 | Good for cloning |
| Cartesia | Per character | $0.015-$0.020 | Ultra-low latency |

#### Telephony Costs

| Component | Typical Cost | Notes |
|-----------|--------------|-------|
| Inbound calls (toll-free) | $0.01-$0.03/min | Varies by country |
| Inbound calls (local) | $0.005-$0.015/min | Geographic numbers |
| Outbound calls (US/Canada) | $0.01-$0.02/min | Higher for mobile |
| Outbound calls (International) | $0.02-$0.50/min | Varies significantly |
| SIP trunking | $0.005-$0.01/min | Volume commitments |
| Phone numbers | $1-5/month each | Per DID |

## Cost Per Conversation Analysis

Understanding the true cost per conversation enables accurate ROI calculations and optimization prioritization.

### Cost Calculator Implementation

```python
class VoiceAICostCalculator:
    """
    Calculate detailed costs for voice AI conversations.
    """

    def __init__(self, pricing_config: PricingConfig):
        self.pricing = pricing_config

    def calculate_conversation_cost(
        self,
        conversation: Conversation
    ) -> ConversationCost:
        """
        Calculate total cost for a completed conversation.
        """
        # STT costs
        stt_cost = self._calculate_stt_cost(conversation)

        # LLM costs
        llm_cost = self._calculate_llm_cost(conversation)

        # TTS costs
        tts_cost = self._calculate_tts_cost(conversation)

        # Telephony costs
        telephony_cost = self._calculate_telephony_cost(conversation)

        # Infrastructure costs (compute, storage, bandwidth)
        infra_cost = self._calculate_infrastructure_cost(conversation)

        total = stt_cost + llm_cost + tts_cost + telephony_cost + infra_cost

        return ConversationCost(
            conversation_id=conversation.id,
            duration_seconds=conversation.duration_seconds,
            stt_cost=stt_cost,
            llm_cost=llm_cost,
            tts_cost=tts_cost,
            telephony_cost=telephony_cost,
            infrastructure_cost=infra_cost,
            total_cost=total,
            cost_per_minute=total / (conversation.duration_seconds / 60),
            breakdown_by_turn=self._calculate_turn_costs(conversation)
        )

    def _calculate_stt_cost(self, conversation: Conversation) -> float:
        """Calculate STT costs based on audio duration"""

        # Total audio duration for user speech
        user_audio_seconds = sum(
            turn.audio_duration_seconds
            for turn in conversation.turns
            if turn.role == 'user'
        )

        # Apply provider pricing
        if self.pricing.stt_provider == 'deepgram':
            # Deepgram charges per minute
            return (user_audio_seconds / 60) * self.pricing.stt_rate_per_minute
        elif self.pricing.stt_provider == 'google':
            # Google charges per 15-second interval
            intervals = math.ceil(user_audio_seconds / 15)
            return intervals * self.pricing.stt_rate_per_interval
        else:
            # Default per-second billing
            return user_audio_seconds * self.pricing.stt_rate_per_second

    def _calculate_llm_cost(self, conversation: Conversation) -> float:
        """Calculate LLM costs based on token usage"""

        total_input_tokens = 0
        total_output_tokens = 0

        for turn in conversation.turns:
            if turn.llm_usage:
                total_input_tokens += turn.llm_usage.input_tokens
                total_output_tokens += turn.llm_usage.output_tokens

        input_cost = (total_input_tokens / 1_000_000) * self.pricing.llm_input_rate
        output_cost = (total_output_tokens / 1_000_000) * self.pricing.llm_output_rate

        return input_cost + output_cost

    def _calculate_tts_cost(self, conversation: Conversation) -> float:
        """Calculate TTS costs based on character count"""

        total_characters = sum(
            len(turn.text)
            for turn in conversation.turns
            if turn.role == 'assistant'
        )

        return (total_characters / 1000) * self.pricing.tts_rate_per_1k_chars

    def _calculate_telephony_cost(self, conversation: Conversation) -> float:
        """Calculate telephony costs"""

        duration_minutes = conversation.duration_seconds / 60

        if conversation.direction == 'inbound':
            rate = self.pricing.inbound_rate_per_minute
        else:
            rate = self.pricing.outbound_rate_per_minute

        return duration_minutes * rate

    def project_monthly_cost(
        self,
        daily_conversations: int,
        avg_duration_minutes: float,
        avg_turns_per_conversation: int
    ) -> MonthlyCostProjection:
        """
        Project monthly costs based on usage estimates.
        """
        # Create synthetic conversation for estimation
        synthetic = self._create_synthetic_conversation(
            duration_minutes=avg_duration_minutes,
            turn_count=avg_turns_per_conversation
        )

        cost_per_conversation = self.calculate_conversation_cost(synthetic).total_cost
        monthly_conversations = daily_conversations * 30

        return MonthlyCostProjection(
            conversations_per_month=monthly_conversations,
            cost_per_conversation=cost_per_conversation,
            total_monthly_cost=cost_per_conversation * monthly_conversations,
            breakdown={
                'stt': self._calculate_stt_cost(synthetic) * monthly_conversations,
                'llm': self._calculate_llm_cost(synthetic) * monthly_conversations,
                'tts': self._calculate_tts_cost(synthetic) * monthly_conversations,
                'telephony': self._calculate_telephony_cost(synthetic) * monthly_conversations,
                'infrastructure': self._calculate_infrastructure_cost(synthetic) * monthly_conversations,
            }
        )
```

### Cost Tracking Dashboard

```python
class CostTrackingSystem:
    """
    Track and analyze voice AI costs in real-time.
    """

    def __init__(self):
        self.cost_calculator = VoiceAICostCalculator()
        self.metrics_store = MetricsStore()
        self.budget_manager = BudgetManager()

    async def track_conversation_cost(
        self,
        conversation: Conversation
    ):
        """
        Track costs for a completed conversation.
        """
        cost = self.cost_calculator.calculate_conversation_cost(conversation)

        # Store cost data
        await self.metrics_store.record_cost(cost)

        # Update running totals
        await self._update_aggregates(cost)

        # Check budget alerts
        await self.budget_manager.check_alerts(cost)

    async def get_cost_report(
        self,
        time_range: TimeRange,
        group_by: str = 'day'
    ) -> CostReport:
        """
        Generate cost report for time period.
        """
        costs = await self.metrics_store.get_costs(time_range)

        return CostReport(
            time_range=time_range,
            total_cost=sum(c.total_cost for c in costs),
            conversation_count=len(costs),
            avg_cost_per_conversation=np.mean([c.total_cost for c in costs]),
            avg_cost_per_minute=np.mean([c.cost_per_minute for c in costs]),
            cost_by_component={
                'stt': sum(c.stt_cost for c in costs),
                'llm': sum(c.llm_cost for c in costs),
                'tts': sum(c.tts_cost for c in costs),
                'telephony': sum(c.telephony_cost for c in costs),
                'infrastructure': sum(c.infrastructure_cost for c in costs),
            },
            trend=self._calculate_cost_trend(costs, group_by),
            anomalies=self._detect_cost_anomalies(costs)
        )
```

## Optimization Strategies

### 1. Caching Strategies

Intelligent caching can dramatically reduce costs for repetitive queries.

```python
class VoiceAICacheManager:
    """
    Multi-layer caching for voice AI cost optimization.
    """

    def __init__(self):
        self.stt_cache = STTCache()
        self.llm_cache = LLMResponseCache()
        self.tts_cache = TTSAudioCache()

    # STT Caching (limited applicability)
    async def get_cached_transcript(
        self,
        audio_hash: str
    ) -> Optional[str]:
        """
        Check for cached transcription (useful for repeated audio like IVR prompts).
        """
        return await self.stt_cache.get(audio_hash)

    # LLM Response Caching
    async def get_cached_llm_response(
        self,
        prompt_hash: str,
        context_hash: str
    ) -> Optional[str]:
        """
        Check for cached LLM response.
        Effective for FAQ-like queries with identical context.
        """
        cache_key = f"{prompt_hash}:{context_hash}"
        cached = await self.llm_cache.get(cache_key)

        if cached and not self._is_expired(cached):
            return cached.response

        return None

    async def cache_llm_response(
        self,
        prompt: str,
        context: List[Dict],
        response: str,
        ttl_seconds: int = 3600
    ):
        """
        Cache an LLM response for future use.
        """
        prompt_hash = self._hash(prompt)
        context_hash = self._hash(json.dumps(context))
        cache_key = f"{prompt_hash}:{context_hash}"

        await self.llm_cache.set(
            cache_key,
            CachedResponse(
                response=response,
                cached_at=datetime.utcnow(),
                ttl=ttl_seconds
            )
        )

    # TTS Audio Caching (high impact)
    async def get_cached_audio(
        self,
        text: str,
        voice_id: str
    ) -> Optional[bytes]:
        """
        Check for cached TTS audio.
        Very effective - same text always produces same audio.
        """
        cache_key = f"{voice_id}:{self._hash(text)}"
        return await self.tts_cache.get(cache_key)

    async def cache_audio(
        self,
        text: str,
        voice_id: str,
        audio: bytes,
        ttl_seconds: int = 86400  # 24 hours
    ):
        """
        Cache synthesized audio.
        """
        cache_key = f"{voice_id}:{self._hash(text)}"
        await self.tts_cache.set(cache_key, audio, ttl=ttl_seconds)


class SemanticLLMCache:
    """
    Semantic caching for LLM responses.
    Returns cached responses for semantically similar queries.
    """

    def __init__(self, similarity_threshold: float = 0.92):
        self.embedder = SentenceEmbedder()
        self.vector_store = VectorStore()
        self.threshold = similarity_threshold

    async def get_semantic_match(
        self,
        query: str,
        context: List[Dict]
    ) -> Optional[CachedResponse]:
        """
        Find semantically similar cached response.
        """
        # Generate embedding for query
        query_embedding = await self.embedder.embed(query)

        # Search for similar queries
        results = await self.vector_store.search(
            query_embedding,
            top_k=5,
            filter={'context_hash': self._hash(json.dumps(context))}
        )

        for result in results:
            if result.similarity >= self.threshold:
                # Verify context match
                if result.metadata['context_hash'] == self._hash(json.dumps(context)):
                    return CachedResponse(
                        response=result.metadata['response'],
                        similarity=result.similarity,
                        original_query=result.metadata['query']
                    )

        return None

    async def store(
        self,
        query: str,
        context: List[Dict],
        response: str
    ):
        """
        Store query-response pair for semantic matching.
        """
        embedding = await self.embedder.embed(query)

        await self.vector_store.upsert(
            id=str(uuid.uuid4()),
            embedding=embedding,
            metadata={
                'query': query,
                'response': response,
                'context_hash': self._hash(json.dumps(context)),
                'created_at': datetime.utcnow().isoformat()
            }
        )
```

### 2. Model Selection Optimization

Choosing the right model for each task significantly impacts costs.

```python
class AdaptiveModelSelector:
    """
    Select optimal model based on query complexity.
    """

    def __init__(self):
        self.complexity_classifier = ComplexityClassifier()
        self.models = {
            'simple': ModelConfig(
                name='gpt-4o-mini',
                cost_per_1k_tokens=0.00015,
                max_tokens=500
            ),
            'medium': ModelConfig(
                name='gpt-4o',
                cost_per_1k_tokens=0.005,
                max_tokens=1000
            ),
            'complex': ModelConfig(
                name='gpt-4o',
                cost_per_1k_tokens=0.005,
                max_tokens=2000
            ),
        }

    async def select_model(
        self,
        query: str,
        context: List[Dict]
    ) -> ModelConfig:
        """
        Select appropriate model based on query complexity.
        """
        # Classify query complexity
        complexity = await self.complexity_classifier.classify(query, context)

        # Simple queries (FAQ, basic info)
        if complexity.score < 0.3:
            return self.models['simple']

        # Medium complexity (multi-step, some reasoning)
        elif complexity.score < 0.7:
            return self.models['medium']

        # Complex queries (analysis, multi-turn reasoning)
        else:
            return self.models['complex']

    async def classify_complexity(
        self,
        query: str,
        context: List[Dict]
    ) -> ComplexityScore:
        """
        Classify query complexity using heuristics and ML.
        """
        # Heuristic features
        word_count = len(query.split())
        question_words = len(re.findall(r'\b(how|why|what if|explain|analyze)\b', query.lower()))
        context_length = sum(len(m['content']) for m in context)

        # ML-based classification
        ml_score = await self._ml_complexity_score(query)

        # Combine signals
        heuristic_score = (
            min(word_count / 50, 1.0) * 0.3 +
            min(question_words / 3, 1.0) * 0.3 +
            min(context_length / 2000, 1.0) * 0.2 +
            ml_score * 0.2
        )

        return ComplexityScore(
            score=heuristic_score,
            factors={
                'word_count': word_count,
                'question_complexity': question_words,
                'context_length': context_length,
                'ml_score': ml_score
            }
        )


class CostAwareRouter:
    """
    Route requests to minimize cost while meeting quality requirements.
    """

    def __init__(self):
        self.model_selector = AdaptiveModelSelector()
        self.quality_monitor = QualityMonitor()

    async def route_request(
        self,
        request: LLMRequest
    ) -> RoutingDecision:
        """
        Route to optimal model balancing cost and quality.
        """
        # Select initial model
        selected_model = await self.model_selector.select_model(
            request.query,
            request.context
        )

        # Check quality history for this query type
        query_type = await self._classify_query_type(request.query)
        quality_history = await self.quality_monitor.get_history(
            query_type,
            selected_model.name
        )

        # If quality is poor, escalate to better model
        if quality_history.avg_score < 0.7:
            selected_model = self._escalate_model(selected_model)

        return RoutingDecision(
            model=selected_model,
            estimated_cost=self._estimate_cost(request, selected_model),
            reasoning=f"Selected {selected_model.name} for {query_type}"
        )
```

### 3. Batching and Request Optimization

```python
class RequestOptimizer:
    """
    Optimize requests to reduce costs.
    """

    def __init__(self):
        self.prompt_optimizer = PromptOptimizer()

    def optimize_prompt(
        self,
        system_prompt: str,
        conversation_history: List[Dict],
        max_context_tokens: int = 2000
    ) -> OptimizedPrompt:
        """
        Optimize prompt to reduce token count while preserving quality.
        """
        # Compress system prompt
        compressed_system = self.prompt_optimizer.compress(system_prompt)

        # Truncate conversation history intelligently
        truncated_history = self._smart_truncate_history(
            conversation_history,
            max_tokens=max_context_tokens
        )

        original_tokens = self._count_tokens(system_prompt) + self._count_tokens(
            json.dumps(conversation_history)
        )
        optimized_tokens = self._count_tokens(compressed_system) + self._count_tokens(
            json.dumps(truncated_history)
        )

        return OptimizedPrompt(
            system_prompt=compressed_system,
            history=truncated_history,
            original_tokens=original_tokens,
            optimized_tokens=optimized_tokens,
            savings_percent=(original_tokens - optimized_tokens) / original_tokens * 100
        )

    def _smart_truncate_history(
        self,
        history: List[Dict],
        max_tokens: int
    ) -> List[Dict]:
        """
        Truncate history keeping most relevant turns.
        """
        # Always keep first turn (establishes context)
        # Always keep last N turns (recent context)
        # Summarize middle turns if needed

        if self._count_tokens(json.dumps(history)) <= max_tokens:
            return history

        # Keep first and last 3 turns
        if len(history) <= 6:
            return history

        first_turn = history[0]
        last_turns = history[-3:]
        middle_turns = history[1:-3]

        # Summarize middle turns
        summary = self._summarize_turns(middle_turns)

        return [
            first_turn,
            {"role": "system", "content": f"[Previous conversation summary: {summary}]"},
            *last_turns
        ]
```

### 4. Self-Hosted vs API Cost Analysis

```python
class DeploymentCostAnalyzer:
    """
    Analyze costs for self-hosted vs API-based deployment.
    """

    def analyze_stt_deployment(
        self,
        monthly_audio_minutes: int,
        peak_concurrent_streams: int
    ) -> DeploymentComparison:
        """
        Compare self-hosted Whisper vs API-based STT.
        """
        # API costs (Deepgram as example)
        api_cost_monthly = monthly_audio_minutes * 0.0043  # Nova model

        # Self-hosted costs
        # Whisper large-v3 needs ~6GB VRAM, can handle ~50 concurrent streams per GPU
        gpus_needed = math.ceil(peak_concurrent_streams / 50)

        # A10G GPU on AWS: ~$1.00/hour
        gpu_hours_monthly = 730  # Always on
        gpu_cost_monthly = gpus_needed * gpu_hours_monthly * 1.00

        # Additional costs
        compute_cost = gpus_needed * 200  # Supporting compute
        storage_cost = 50  # Model storage
        engineering_cost = 500  # Maintenance overhead

        self_hosted_total = gpu_cost_monthly + compute_cost + storage_cost + engineering_cost

        # Break-even analysis
        break_even_minutes = self_hosted_total / 0.0043

        return DeploymentComparison(
            api_monthly_cost=api_cost_monthly,
            self_hosted_monthly_cost=self_hosted_total,
            break_even_minutes=break_even_minutes,
            recommendation='self_hosted' if monthly_audio_minutes > break_even_minutes else 'api',
            details={
                'gpus_required': gpus_needed,
                'api_rate': 0.0043,
                'self_hosted_effective_rate': self_hosted_total / monthly_audio_minutes
                    if monthly_audio_minutes > 0 else float('inf')
            }
        )

    def analyze_llm_deployment(
        self,
        monthly_requests: int,
        avg_input_tokens: int,
        avg_output_tokens: int
    ) -> DeploymentComparison:
        """
        Compare self-hosted LLM vs API-based.
        """
        # API costs (GPT-4o as baseline)
        total_input_tokens = monthly_requests * avg_input_tokens
        total_output_tokens = monthly_requests * avg_output_tokens

        api_cost = (
            (total_input_tokens / 1_000_000) * 2.50 +
            (total_output_tokens / 1_000_000) * 10.00
        )

        # Self-hosted Llama 70B costs
        # Needs 2x A100 80GB for good performance
        gpu_cost = 2 * 730 * 4.00  # ~$5,840/month for GPUs
        compute_cost = 500
        engineering_cost = 1000

        self_hosted_total = gpu_cost + compute_cost + engineering_cost

        # Self-hosted can handle ~10 requests/second
        max_monthly_requests = 10 * 60 * 60 * 24 * 30  # ~26M requests

        return DeploymentComparison(
            api_monthly_cost=api_cost,
            self_hosted_monthly_cost=self_hosted_total,
            break_even_requests=self_hosted_total / (api_cost / monthly_requests)
                if monthly_requests > 0 else float('inf'),
            recommendation=self._recommend_llm_deployment(
                api_cost, self_hosted_total, monthly_requests, max_monthly_requests
            ),
            details={
                'api_cost_per_request': api_cost / monthly_requests if monthly_requests > 0 else 0,
                'self_hosted_cost_per_request': self_hosted_total / monthly_requests
                    if monthly_requests > 0 else float('inf'),
                'self_hosted_max_capacity': max_monthly_requests
            }
        )
```

## Build vs Buy Analysis

Deciding whether to build or buy voice AI components requires careful analysis of costs, capabilities, and strategic considerations.

### Build vs Buy Framework

```
+-----------------------------------------------------------------------------+
|                    BUILD VS BUY DECISION FRAMEWORK                            |
+-----------------------------------------------------------------------------+
|                                                                               |
|  COMPONENT: VOICE AI PLATFORM                                                 |
|                                                                               |
|  FACTORS FAVORING BUILD                 FACTORS FAVORING BUY                 |
|  +--------------------------------+    +--------------------------------+     |
|  | - High call volume (>1M/month) |    | - Faster time to market        |     |
|  | - Unique requirements          |    | - Lower upfront investment     |     |
|  | - Data privacy requirements    |    | - Access to latest models      |     |
|  | - Competitive differentiation  |    | - Reduced maintenance burden   |     |
|  | - Long-term cost advantage     |    | - Flexible scaling             |     |
|  | - In-house ML expertise        |    | - No ML expertise needed       |     |
|  +--------------------------------+    +--------------------------------+     |
|                                                                               |
|  HYBRID APPROACH (Recommended for most)                                       |
|  +----------------------------------------------------------------------+    |
|  | - Buy: Telephony, basic STT/TTS, LLM APIs                            |    |
|  | - Build: Orchestration, business logic, custom models                |    |
|  | - Self-host: High-volume components with stable requirements         |    |
|  +----------------------------------------------------------------------+    |
|                                                                               |
+-----------------------------------------------------------------------------+
```

### Build vs Buy Cost Model

```python
class BuildVsBuyAnalyzer:
    """
    Comprehensive build vs buy analysis for voice AI.
    """

    def analyze(
        self,
        requirements: VoiceAIRequirements,
        time_horizon_years: int = 3
    ) -> BuildVsBuyAnalysis:
        """
        Analyze build vs buy decision with TCO.
        """
        # BUY scenario costs
        buy_costs = self._calculate_buy_costs(requirements, time_horizon_years)

        # BUILD scenario costs
        build_costs = self._calculate_build_costs(requirements, time_horizon_years)

        # Calculate NPV for comparison
        discount_rate = 0.10  # 10% discount rate
        buy_npv = self._calculate_npv(buy_costs.yearly_costs, discount_rate)
        build_npv = self._calculate_npv(build_costs.yearly_costs, discount_rate)

        return BuildVsBuyAnalysis(
            buy_total_cost=sum(buy_costs.yearly_costs),
            build_total_cost=sum(build_costs.yearly_costs),
            buy_npv=buy_npv,
            build_npv=build_npv,
            break_even_month=self._find_break_even(buy_costs, build_costs),
            recommendation=self._generate_recommendation(
                buy_costs, build_costs, requirements
            ),
            risk_factors=self._assess_risks(requirements)
        )

    def _calculate_buy_costs(
        self,
        requirements: VoiceAIRequirements,
        years: int
    ) -> CostProjection:
        """
        Calculate total cost of ownership for buy scenario.
        """
        yearly_costs = []

        for year in range(years):
            # Volume grows over time
            volume_multiplier = 1 + (requirements.growth_rate * year)
            monthly_calls = requirements.monthly_calls * volume_multiplier

            # Per-call costs (API pricing)
            per_call_cost = (
                0.02 +  # STT
                0.03 +  # LLM
                0.015 + # TTS
                0.01    # Telephony
            )  # Total: ~$0.075/call

            annual_variable_cost = monthly_calls * 12 * per_call_cost

            # Platform fees (typical SaaS pricing)
            platform_fee = 5000 * 12  # $5K/month base

            # Integration and maintenance
            integration_cost = 20000 if year == 0 else 5000

            yearly_costs.append(
                annual_variable_cost + platform_fee + integration_cost
            )

        return CostProjection(
            yearly_costs=yearly_costs,
            components={
                'variable': [y * 0.7 for y in yearly_costs],
                'platform': [60000] * years,
                'integration': [20000] + [5000] * (years - 1)
            }
        )

    def _calculate_build_costs(
        self,
        requirements: VoiceAIRequirements,
        years: int
    ) -> CostProjection:
        """
        Calculate total cost of ownership for build scenario.
        """
        yearly_costs = []

        # Year 0: Heavy development investment
        year_0_costs = {
            'development': 500000,  # Engineering team
            'infrastructure': 100000,  # Initial setup
            'training_data': 50000,
            'tools_and_licenses': 30000
        }

        yearly_costs.append(sum(year_0_costs.values()))

        # Subsequent years
        for year in range(1, years):
            volume_multiplier = 1 + (requirements.growth_rate * year)
            monthly_calls = requirements.monthly_calls * volume_multiplier

            # Infrastructure scales with volume
            infrastructure_cost = self._estimate_infrastructure_cost(monthly_calls)

            # Ongoing development and maintenance
            maintenance_cost = 200000  # 2 FTEs

            # Lower per-call cost for self-hosted
            per_call_cost = 0.02  # Mainly telephony + compute
            variable_cost = monthly_calls * 12 * per_call_cost

            yearly_costs.append(
                infrastructure_cost + maintenance_cost + variable_cost
            )

        return CostProjection(
            yearly_costs=yearly_costs,
            components={
                'development': [500000] + [200000] * (years - 1),
                'infrastructure': [100000] + [
                    self._estimate_infrastructure_cost(
                        requirements.monthly_calls * (1 + requirements.growth_rate * y)
                    )
                    for y in range(1, years)
                ],
                'variable': [0] + [
                    requirements.monthly_calls * (1 + requirements.growth_rate * y) * 12 * 0.02
                    for y in range(1, years)
                ]
            }
        )
```

## ROI Calculation Framework

Calculating ROI for voice AI investments requires comparing costs against measurable benefits.

### ROI Calculator

```python
class VoiceAIROICalculator:
    """
    Calculate ROI for voice AI implementation.
    """

    def calculate_roi(
        self,
        current_state: CurrentOperations,
        projected_state: VoiceAIProjection,
        implementation_cost: float,
        time_horizon_months: int = 12
    ) -> ROIAnalysis:
        """
        Calculate comprehensive ROI for voice AI investment.
        """
        # Current costs (human agents)
        current_monthly_cost = self._calculate_current_costs(current_state)

        # Projected costs (voice AI + humans)
        projected_monthly_cost = self._calculate_projected_costs(
            current_state,
            projected_state
        )

        # Monthly savings
        monthly_savings = current_monthly_cost - projected_monthly_cost

        # Calculate ROI metrics
        total_savings = monthly_savings * time_horizon_months
        net_benefit = total_savings - implementation_cost
        roi_percent = (net_benefit / implementation_cost) * 100
        payback_months = implementation_cost / monthly_savings if monthly_savings > 0 else float('inf')

        return ROIAnalysis(
            implementation_cost=implementation_cost,
            current_monthly_cost=current_monthly_cost,
            projected_monthly_cost=projected_monthly_cost,
            monthly_savings=monthly_savings,
            total_savings_over_period=total_savings,
            net_benefit=net_benefit,
            roi_percent=roi_percent,
            payback_period_months=payback_months,
            benefits_breakdown=self._calculate_benefits_breakdown(
                current_state, projected_state
            )
        )

    def _calculate_current_costs(
        self,
        current: CurrentOperations
    ) -> float:
        """
        Calculate current monthly operating costs.
        """
        # Agent costs
        agent_cost = (
            current.num_agents *
            current.avg_agent_salary_monthly *
            (1 + current.benefits_multiplier)  # Include benefits
        )

        # Management overhead
        management_cost = agent_cost * 0.15  # ~15% overhead

        # Telephony costs
        telephony_cost = current.monthly_calls * current.avg_call_duration_min * 0.015

        # Training and turnover costs
        turnover_cost = (
            current.num_agents *
            current.annual_turnover_rate / 12 *
            current.training_cost_per_agent
        )

        return agent_cost + management_cost + telephony_cost + turnover_cost

    def _calculate_projected_costs(
        self,
        current: CurrentOperations,
        projected: VoiceAIProjection
    ) -> float:
        """
        Calculate projected monthly costs with voice AI.
        """
        # Calls handled by AI (no agent cost)
        ai_handled_calls = current.monthly_calls * projected.containment_rate
        ai_cost = ai_handled_calls * projected.cost_per_ai_call

        # Calls requiring human agents
        human_handled_calls = current.monthly_calls * (1 - projected.containment_rate)

        # Reduced agent headcount
        calls_per_agent = current.monthly_calls / current.num_agents
        required_agents = math.ceil(human_handled_calls / calls_per_agent)

        agent_cost = (
            required_agents *
            current.avg_agent_salary_monthly *
            (1 + current.benefits_multiplier)
        )

        # Voice AI platform costs
        platform_cost = projected.monthly_platform_cost

        # Reduced management overhead
        management_cost = agent_cost * 0.10  # Lower with AI assistance

        return ai_cost + agent_cost + platform_cost + management_cost

    def _calculate_benefits_breakdown(
        self,
        current: CurrentOperations,
        projected: VoiceAIProjection
    ) -> Dict[str, float]:
        """
        Break down benefits by category.
        """
        current_cost = self._calculate_current_costs(current)
        projected_cost = self._calculate_projected_costs(current, projected)
        total_savings = current_cost - projected_cost

        # Estimate breakdown
        agent_reduction = current.num_agents - math.ceil(
            current.num_agents * (1 - projected.containment_rate)
        )

        return {
            'labor_cost_reduction': agent_reduction * current.avg_agent_salary_monthly * 1.25,
            'reduced_turnover_cost': agent_reduction * current.training_cost_per_agent * current.annual_turnover_rate / 12,
            'improved_efficiency': total_savings * 0.1,  # Faster handle times
            'extended_hours_coverage': projected.after_hours_value,  # 24/7 without overtime
            'reduced_wait_times': projected.customer_satisfaction_value,
        }


# Example ROI calculation
def example_roi_calculation():
    """
    Example: Calculate ROI for a mid-size contact center.
    """
    current = CurrentOperations(
        num_agents=50,
        avg_agent_salary_monthly=4500,
        benefits_multiplier=0.25,  # 25% benefits cost
        monthly_calls=100000,
        avg_call_duration_min=5,
        annual_turnover_rate=0.30,  # 30% turnover
        training_cost_per_agent=5000
    )

    projected = VoiceAIProjection(
        containment_rate=0.65,  # AI handles 65% of calls
        cost_per_ai_call=0.08,  # $0.08 per AI-handled call
        monthly_platform_cost=10000,
        after_hours_value=5000,  # Value of 24/7 coverage
        customer_satisfaction_value=3000  # CSAT improvement value
    )

    calculator = VoiceAIROICalculator()
    roi = calculator.calculate_roi(
        current_state=current,
        projected_state=projected,
        implementation_cost=300000,  # $300K implementation
        time_horizon_months=24
    )

    print(f"Monthly savings: ${roi.monthly_savings:,.0f}")
    print(f"Payback period: {roi.payback_period_months:.1f} months")
    print(f"24-month ROI: {roi.roi_percent:.1f}%")
```

### ROI Sensitivity Analysis

```python
class ROISensitivityAnalyzer:
    """
    Analyze sensitivity of ROI to key assumptions.
    """

    def analyze_sensitivity(
        self,
        base_case: ROIAnalysis,
        variables: List[SensitivityVariable]
    ) -> SensitivityAnalysis:
        """
        Analyze how ROI changes with different assumptions.
        """
        results = []

        for variable in variables:
            variable_results = []

            for variation in variable.variations:
                # Calculate ROI with this variation
                modified_case = self._apply_variation(base_case, variable, variation)
                new_roi = self._recalculate_roi(modified_case)

                variable_results.append({
                    'variation': variation,
                    'roi_percent': new_roi.roi_percent,
                    'payback_months': new_roi.payback_period_months,
                    'net_benefit': new_roi.net_benefit
                })

            results.append(SensitivityResult(
                variable_name=variable.name,
                base_value=variable.base_value,
                variations=variable_results,
                impact_score=self._calculate_impact(variable_results, base_case)
            ))

        return SensitivityAnalysis(
            base_case=base_case,
            sensitivity_results=results,
            tornado_chart_data=self._generate_tornado_data(results),
            recommendations=self._generate_recommendations(results)
        )

    # Key variables to test
    SENSITIVITY_VARIABLES = [
        SensitivityVariable(
            name='containment_rate',
            base_value=0.65,
            variations=[0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80],
            description='Percentage of calls handled by AI'
        ),
        SensitivityVariable(
            name='cost_per_ai_call',
            base_value=0.08,
            variations=[0.05, 0.06, 0.08, 0.10, 0.12, 0.15],
            description='Cost per AI-handled call'
        ),
        SensitivityVariable(
            name='call_volume_growth',
            base_value=0.10,
            variations=[-0.10, 0, 0.10, 0.20, 0.30],
            description='Annual call volume growth rate'
        ),
        SensitivityVariable(
            name='implementation_cost',
            base_value=300000,
            variations=[200000, 250000, 300000, 400000, 500000],
            description='Total implementation cost'
        ),
    ]
```

## Cost Monitoring and Alerts

Continuous cost monitoring prevents unexpected expenses and identifies optimization opportunities.

```python
class CostMonitoringSystem:
    """
    Monitor voice AI costs and alert on anomalies.
    """

    def __init__(self):
        self.budget_tracker = BudgetTracker()
        self.anomaly_detector = CostAnomalyDetector()
        self.alert_manager = AlertManager()

    async def monitor_costs(self):
        """
        Continuous cost monitoring loop.
        """
        while True:
            # Get current costs
            current_costs = await self._get_current_costs()

            # Check budget thresholds
            await self._check_budgets(current_costs)

            # Detect anomalies
            anomalies = await self.anomaly_detector.detect(current_costs)
            for anomaly in anomalies:
                await self._handle_anomaly(anomaly)

            # Check for optimization opportunities
            opportunities = await self._identify_optimizations(current_costs)
            if opportunities:
                await self._notify_opportunities(opportunities)

            await asyncio.sleep(300)  # Check every 5 minutes

    async def _check_budgets(self, costs: CurrentCosts):
        """
        Check costs against budget thresholds.
        """
        # Daily budget check
        if costs.daily_total > self.budget_tracker.daily_limit * 0.8:
            await self.alert_manager.send(
                AlertType.BUDGET_WARNING,
                f"Daily spend at {costs.daily_total / self.budget_tracker.daily_limit * 100:.0f}% of limit",
                severity='medium'
            )

        if costs.daily_total > self.budget_tracker.daily_limit:
            await self.alert_manager.send(
                AlertType.BUDGET_EXCEEDED,
                f"Daily budget exceeded: ${costs.daily_total:.2f} > ${self.budget_tracker.daily_limit:.2f}",
                severity='high'
            )

        # Monthly budget check
        monthly_projection = costs.month_to_date + (
            costs.daily_average * self._days_remaining_in_month()
        )

        if monthly_projection > self.budget_tracker.monthly_limit * 1.1:
            await self.alert_manager.send(
                AlertType.BUDGET_PROJECTION_WARNING,
                f"Projected monthly spend: ${monthly_projection:,.0f} (budget: ${self.budget_tracker.monthly_limit:,.0f})",
                severity='medium'
            )

    async def _identify_optimizations(
        self,
        costs: CurrentCosts
    ) -> List[OptimizationOpportunity]:
        """
        Identify potential cost optimizations.
        """
        opportunities = []

        # Check LLM model usage patterns
        if costs.llm_costs.complex_model_usage_rate > 0.7:
            # Too many calls going to expensive model
            potential_savings = costs.llm_costs.total * 0.3  # Estimate 30% savings
            opportunities.append(OptimizationOpportunity(
                category='model_selection',
                description='High usage of expensive LLM model - consider adaptive routing',
                potential_monthly_savings=potential_savings,
                implementation_effort='medium'
            ))

        # Check TTS caching effectiveness
        if costs.tts_costs.cache_hit_rate < 0.3:
            potential_savings = costs.tts_costs.total * 0.2
            opportunities.append(OptimizationOpportunity(
                category='caching',
                description='Low TTS cache hit rate - review caching strategy',
                potential_monthly_savings=potential_savings,
                implementation_effort='low'
            ))

        # Check for self-hosting candidates
        if costs.stt_costs.total > 5000:  # Monthly STT > $5K
            # May be worth self-hosting
            opportunities.append(OptimizationOpportunity(
                category='infrastructure',
                description='High STT costs - evaluate self-hosted Whisper',
                potential_monthly_savings=costs.stt_costs.total * 0.5,
                implementation_effort='high'
            ))

        return opportunities
```

## Summary

Effective voice AI cost optimization requires:

1. **Understanding cost components**: Know where money goes across STT, LLM, TTS, and telephony
2. **Implementing caching**: Cache TTS audio and semantically similar LLM responses
3. **Adaptive model selection**: Use cheaper models for simple queries, expensive models only when needed
4. **Optimizing prompts**: Reduce token usage through compression and smart history truncation
5. **Build vs buy analysis**: Make informed decisions based on volume, expertise, and strategic value
6. **ROI calculation**: Quantify benefits against implementation and operating costs
7. **Continuous monitoring**: Track costs in real-time and alert on anomalies

The most effective optimization strategy combines multiple approaches: caching for TTS (high impact, low effort), adaptive model selection for LLMs (medium impact, medium effort), and strategic self-hosting for high-volume components (high impact, high effort).

Remember that cost optimization should not compromise user experience. Monitor quality metrics alongside costs to ensure optimizations do not degrade conversation quality or customer satisfaction.
