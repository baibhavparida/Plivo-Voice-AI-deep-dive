---
title: "gRPC Streaming for Voice AI"
description: "Implementing high-performance bidirectional streaming with gRPC for real-time voice AI applications"
---

# gRPC Streaming for Voice AI

gRPC (Google Remote Procedure Call) offers superior performance for voice AI applications through HTTP/2, protocol buffers, and native bidirectional streaming. This guide covers gRPC streaming patterns, implementation strategies, and optimization techniques for voice AI systems.

## Why gRPC for Voice AI?

gRPC excels in voice AI scenarios requiring low latency and high throughput:

| Feature | gRPC | REST/HTTP | WebSocket |
|---------|------|-----------|-----------|
| **Protocol** | HTTP/2 | HTTP/1.1 | HTTP/1.1 upgrade |
| **Serialization** | Protocol Buffers | JSON | Custom |
| **Streaming** | Native bidirectional | Response only | Full-duplex |
| **Multiplexing** | Yes | No | Per connection |
| **Header Compression** | HPACK | None | None |
| **Type Safety** | Strong | None | None |
| **Code Generation** | Automatic | Manual | Manual |

### Performance Benefits

```
Latency Comparison (audio frame transmission):
├── gRPC + Protobuf:    ~2-5ms
├── WebSocket + JSON:   ~5-15ms
└── REST + JSON:        ~20-50ms

Bandwidth Comparison (same audio data):
├── gRPC + Protobuf:    100% (baseline)
├── WebSocket + JSON:   150-200%
└── REST + JSON:        200-300%
```

## Protocol Buffer Definitions

### Voice AI Service Definition

```protobuf
syntax = "proto3";

package voiceai.v1;

import "google/protobuf/duration.proto";
import "google/protobuf/timestamp.proto";

// Main Voice AI service
service VoiceAIService {
  // Bidirectional streaming for real-time voice conversation
  rpc StreamingConversation(stream AudioRequest)
      returns (stream ConversationResponse);

  // Server-side streaming for TTS
  rpc SynthesizeSpeech(SynthesisRequest)
      returns (stream AudioChunk);

  // Client-side streaming for STT
  rpc RecognizeSpeech(stream AudioChunk)
      returns (TranscriptionResult);

  // Unary call for configuration
  rpc GetSessionConfig(SessionConfigRequest)
      returns (SessionConfig);
}

// Audio configuration
message AudioConfig {
  enum Encoding {
    ENCODING_UNSPECIFIED = 0;
    LINEAR16 = 1;
    MULAW = 2;
    ALAW = 3;
    OPUS = 4;
    FLAC = 5;
  }

  Encoding encoding = 1;
  int32 sample_rate_hertz = 2;
  int32 channel_count = 3;
  bool enable_preprocessing = 4;
}

// Recognition configuration
message RecognitionConfig {
  string model = 1;
  string language_code = 2;
  bool interim_results = 3;
  bool enable_punctuation = 4;
  bool enable_word_timestamps = 5;
  repeated string phrase_hints = 6;
  float boost = 7;
}

// Synthesis configuration
message SynthesisConfig {
  string voice_id = 1;
  float speaking_rate = 2;
  float pitch = 3;
  float volume_gain_db = 4;
  AudioConfig output_audio_config = 5;
}

// Client sends audio and control messages
message AudioRequest {
  oneof request {
    SessionStart session_start = 1;
    AudioChunk audio = 2;
    ControlSignal control = 3;
  }
}

message SessionStart {
  AudioConfig audio_config = 1;
  RecognitionConfig recognition_config = 2;
  SynthesisConfig synthesis_config = 3;
  map<string, string> metadata = 4;
}

message AudioChunk {
  bytes audio_content = 1;
  google.protobuf.Timestamp timestamp = 2;
  int64 sequence_number = 3;
}

message ControlSignal {
  enum SignalType {
    SIGNAL_UNSPECIFIED = 0;
    END_OF_SPEECH = 1;
    CANCEL = 2;
    BARGE_IN = 3;
    RESET = 4;
  }
  SignalType signal = 1;
  google.protobuf.Timestamp timestamp = 2;
}

// Server sends transcripts, responses, and audio
message ConversationResponse {
  oneof response {
    TranscriptionResult transcription = 1;
    LLMResponse llm_response = 2;
    AudioChunk audio = 3;
    StatusUpdate status = 4;
    ErrorResponse error = 5;
  }
}

message TranscriptionResult {
  string transcript = 1;
  float confidence = 2;
  bool is_final = 3;
  repeated WordInfo words = 4;
  google.protobuf.Duration audio_duration = 5;
}

message WordInfo {
  string word = 1;
  google.protobuf.Duration start_time = 2;
  google.protobuf.Duration end_time = 3;
  float confidence = 4;
}

message LLMResponse {
  string text = 1;
  bool is_complete = 2;
  string function_call = 3;
  map<string, string> metadata = 4;
}

message StatusUpdate {
  enum State {
    STATE_UNSPECIFIED = 0;
    LISTENING = 1;
    PROCESSING = 2;
    SPEAKING = 3;
    IDLE = 4;
  }
  State state = 1;
  LatencyMetrics latency = 2;
}

message LatencyMetrics {
  int32 stt_ms = 1;
  int32 llm_ms = 2;
  int32 tts_ms = 3;
  int32 total_ms = 4;
}

message ErrorResponse {
  int32 code = 1;
  string message = 2;
  bool recoverable = 3;
}

// TTS-specific messages
message SynthesisRequest {
  string text = 1;
  SynthesisConfig config = 2;
}

message SessionConfigRequest {
  string session_id = 1;
}

message SessionConfig {
  string session_id = 1;
  AudioConfig audio_config = 2;
  RecognitionConfig recognition_config = 3;
  SynthesisConfig synthesis_config = 4;
  int64 max_audio_duration_seconds = 5;
}
```

## Server Implementation

### Python gRPC Server

```python
import grpc
from concurrent import futures
import asyncio
from typing import AsyncIterator, Iterator

import voice_ai_pb2
import voice_ai_pb2_grpc

class VoiceAIServicer(voice_ai_pb2_grpc.VoiceAIServiceServicer):
    def __init__(self, stt_client, llm_client, tts_client):
        self.stt_client = stt_client
        self.llm_client = llm_client
        self.tts_client = tts_client

    async def StreamingConversation(
        self,
        request_iterator: AsyncIterator[voice_ai_pb2.AudioRequest],
        context: grpc.aio.ServicerContext
    ) -> AsyncIterator[voice_ai_pb2.ConversationResponse]:
        """
        Bidirectional streaming for real-time conversation.
        Receives audio, returns transcripts, LLM responses, and TTS audio.
        """
        session_config = None
        audio_buffer = bytearray()

        try:
            async for request in request_iterator:
                request_type = request.WhichOneof('request')

                if request_type == 'session_start':
                    session_config = request.session_start
                    yield voice_ai_pb2.ConversationResponse(
                        status=voice_ai_pb2.StatusUpdate(
                            state=voice_ai_pb2.StatusUpdate.LISTENING
                        )
                    )

                elif request_type == 'audio':
                    audio_buffer.extend(request.audio.audio_content)

                    # Stream to STT for interim results
                    async for transcript in self._stream_stt(
                        request.audio.audio_content, session_config
                    ):
                        yield voice_ai_pb2.ConversationResponse(
                            transcription=transcript
                        )

                elif request_type == 'control':
                    if request.control.signal == voice_ai_pb2.ControlSignal.END_OF_SPEECH:
                        # Process complete utterance
                        async for response in self._process_utterance(
                            bytes(audio_buffer), session_config
                        ):
                            yield response
                        audio_buffer.clear()

                    elif request.control.signal == voice_ai_pb2.ControlSignal.BARGE_IN:
                        # Handle interruption
                        yield voice_ai_pb2.ConversationResponse(
                            status=voice_ai_pb2.StatusUpdate(
                                state=voice_ai_pb2.StatusUpdate.LISTENING
                            )
                        )

        except grpc.aio.AioRpcError as e:
            yield voice_ai_pb2.ConversationResponse(
                error=voice_ai_pb2.ErrorResponse(
                    code=e.code().value[0],
                    message=str(e.details()),
                    recoverable=self._is_recoverable(e.code())
                )
            )

    async def _stream_stt(
        self,
        audio_chunk: bytes,
        config: voice_ai_pb2.SessionStart
    ) -> AsyncIterator[voice_ai_pb2.TranscriptionResult]:
        """Stream audio to STT and yield interim results."""
        async for result in self.stt_client.streaming_recognize(
            audio_chunk,
            sample_rate=config.audio_config.sample_rate_hertz,
            encoding=config.audio_config.encoding,
            language=config.recognition_config.language_code,
            interim_results=config.recognition_config.interim_results
        ):
            yield voice_ai_pb2.TranscriptionResult(
                transcript=result.text,
                confidence=result.confidence,
                is_final=result.is_final,
                words=[
                    voice_ai_pb2.WordInfo(
                        word=w.word,
                        start_time=w.start_time,
                        end_time=w.end_time,
                        confidence=w.confidence
                    ) for w in result.words
                ]
            )

    async def _process_utterance(
        self,
        audio_data: bytes,
        config: voice_ai_pb2.SessionStart
    ) -> AsyncIterator[voice_ai_pb2.ConversationResponse]:
        """Process complete utterance through STT -> LLM -> TTS."""
        # Update status
        yield voice_ai_pb2.ConversationResponse(
            status=voice_ai_pb2.StatusUpdate(
                state=voice_ai_pb2.StatusUpdate.PROCESSING
            )
        )

        # Get final transcription
        final_transcript = await self.stt_client.recognize(audio_data)
        yield voice_ai_pb2.ConversationResponse(
            transcription=voice_ai_pb2.TranscriptionResult(
                transcript=final_transcript.text,
                confidence=final_transcript.confidence,
                is_final=True
            )
        )

        # Stream LLM response
        full_response = ""
        async for chunk in self.llm_client.stream_completion(final_transcript.text):
            full_response += chunk
            yield voice_ai_pb2.ConversationResponse(
                llm_response=voice_ai_pb2.LLMResponse(
                    text=chunk,
                    is_complete=False
                )
            )

        yield voice_ai_pb2.ConversationResponse(
            llm_response=voice_ai_pb2.LLMResponse(
                text="",
                is_complete=True
            )
        )

        # Update status to speaking
        yield voice_ai_pb2.ConversationResponse(
            status=voice_ai_pb2.StatusUpdate(
                state=voice_ai_pb2.StatusUpdate.SPEAKING
            )
        )

        # Stream TTS audio
        async for audio_chunk in self.tts_client.stream_synthesis(
            full_response,
            voice_id=config.synthesis_config.voice_id,
            speaking_rate=config.synthesis_config.speaking_rate
        ):
            yield voice_ai_pb2.ConversationResponse(
                audio=voice_ai_pb2.AudioChunk(
                    audio_content=audio_chunk
                )
            )

        # Return to listening
        yield voice_ai_pb2.ConversationResponse(
            status=voice_ai_pb2.StatusUpdate(
                state=voice_ai_pb2.StatusUpdate.LISTENING
            )
        )

    async def SynthesizeSpeech(
        self,
        request: voice_ai_pb2.SynthesisRequest,
        context: grpc.aio.ServicerContext
    ) -> AsyncIterator[voice_ai_pb2.AudioChunk]:
        """Server-side streaming for TTS."""
        async for chunk in self.tts_client.stream_synthesis(
            request.text,
            voice_id=request.config.voice_id,
            speaking_rate=request.config.speaking_rate
        ):
            yield voice_ai_pb2.AudioChunk(audio_content=chunk)

    async def RecognizeSpeech(
        self,
        request_iterator: AsyncIterator[voice_ai_pb2.AudioChunk],
        context: grpc.aio.ServicerContext
    ) -> voice_ai_pb2.TranscriptionResult:
        """Client-side streaming for STT."""
        audio_data = bytearray()

        async for chunk in request_iterator:
            audio_data.extend(chunk.audio_content)

        result = await self.stt_client.recognize(bytes(audio_data))
        return voice_ai_pb2.TranscriptionResult(
            transcript=result.text,
            confidence=result.confidence,
            is_final=True
        )


async def serve():
    server = grpc.aio.server(
        futures.ThreadPoolExecutor(max_workers=10),
        options=[
            ('grpc.max_send_message_length', 50 * 1024 * 1024),
            ('grpc.max_receive_message_length', 50 * 1024 * 1024),
            ('grpc.keepalive_time_ms', 30000),
            ('grpc.keepalive_timeout_ms', 10000),
            ('grpc.keepalive_permit_without_calls', True),
            ('grpc.http2.max_pings_without_data', 0),
        ]
    )

    voice_ai_pb2_grpc.add_VoiceAIServiceServicer_to_server(
        VoiceAIServicer(stt, llm, tts), server
    )

    server.add_insecure_port('[::]:50051')
    await server.start()
    await server.wait_for_termination()


if __name__ == '__main__':
    asyncio.run(serve())
```

## Client Implementation

### Python Async Client

```python
import grpc
import asyncio
from typing import AsyncIterator

import voice_ai_pb2
import voice_ai_pb2_grpc

class VoiceAIClient:
    def __init__(self, host: str = 'localhost', port: int = 50051):
        self.channel = grpc.aio.insecure_channel(
            f'{host}:{port}',
            options=[
                ('grpc.keepalive_time_ms', 30000),
                ('grpc.keepalive_timeout_ms', 10000),
            ]
        )
        self.stub = voice_ai_pb2_grpc.VoiceAIServiceStub(self.channel)

    async def streaming_conversation(
        self,
        audio_stream: AsyncIterator[bytes],
        config: dict
    ) -> AsyncIterator[voice_ai_pb2.ConversationResponse]:
        """
        Start a bidirectional streaming conversation.
        """
        async def request_generator():
            # Send session start
            yield voice_ai_pb2.AudioRequest(
                session_start=voice_ai_pb2.SessionStart(
                    audio_config=voice_ai_pb2.AudioConfig(
                        encoding=voice_ai_pb2.AudioConfig.LINEAR16,
                        sample_rate_hertz=config.get('sample_rate', 16000),
                        channel_count=1
                    ),
                    recognition_config=voice_ai_pb2.RecognitionConfig(
                        model=config.get('stt_model', 'nova-2'),
                        language_code=config.get('language', 'en-US'),
                        interim_results=True
                    ),
                    synthesis_config=voice_ai_pb2.SynthesisConfig(
                        voice_id=config.get('voice_id', 'default'),
                        speaking_rate=config.get('speaking_rate', 1.0)
                    )
                )
            )

            # Stream audio
            async for audio_chunk in audio_stream:
                yield voice_ai_pb2.AudioRequest(
                    audio=voice_ai_pb2.AudioChunk(
                        audio_content=audio_chunk
                    )
                )

        # Start bidirectional stream
        response_stream = self.stub.StreamingConversation(
            request_generator()
        )

        async for response in response_stream:
            yield response

    async def synthesize_speech(
        self,
        text: str,
        voice_id: str = 'default'
    ) -> AsyncIterator[bytes]:
        """Stream TTS synthesis."""
        request = voice_ai_pb2.SynthesisRequest(
            text=text,
            config=voice_ai_pb2.SynthesisConfig(voice_id=voice_id)
        )

        async for chunk in self.stub.SynthesizeSpeech(request):
            yield chunk.audio_content

    async def close(self):
        await self.channel.close()


# Usage example
async def main():
    client = VoiceAIClient()

    # Simulate audio stream from microphone
    async def audio_generator():
        # In reality, this would capture from microphone
        for i in range(100):
            yield b'\x00' * 3200  # 100ms of silence at 16kHz
            await asyncio.sleep(0.1)

    config = {
        'sample_rate': 16000,
        'language': 'en-US',
        'voice_id': 'alloy'
    }

    async for response in client.streaming_conversation(audio_generator(), config):
        response_type = response.WhichOneof('response')

        if response_type == 'transcription':
            print(f"Transcript: {response.transcription.transcript}")
        elif response_type == 'llm_response':
            print(f"LLM: {response.llm_response.text}", end='')
        elif response_type == 'audio':
            print(f"[Audio chunk: {len(response.audio.audio_content)} bytes]")
        elif response_type == 'status':
            print(f"Status: {response.status.state}")

    await client.close()


asyncio.run(main())
```

### TypeScript/Node.js Client

```typescript
import * as grpc from '@grpc/grpc-js';
import * as protoLoader from '@grpc/proto-loader';
import { Transform, Readable } from 'stream';

const packageDefinition = protoLoader.loadSync('voice_ai.proto', {
  keepCase: true,
  longs: String,
  enums: String,
  defaults: true,
  oneofs: true
});

const voiceAiProto = grpc.loadPackageDefinition(packageDefinition).voiceai.v1;

class VoiceAIClient {
  private client: any;

  constructor(host: string = 'localhost:50051') {
    this.client = new voiceAiProto.VoiceAIService(
      host,
      grpc.credentials.createInsecure(),
      {
        'grpc.keepalive_time_ms': 30000,
        'grpc.keepalive_timeout_ms': 10000,
      }
    );
  }

  streamingConversation(config: any): grpc.ClientDuplexStream<any, any> {
    const call = this.client.StreamingConversation();

    // Send session start
    call.write({
      session_start: {
        audio_config: {
          encoding: 'LINEAR16',
          sample_rate_hertz: config.sampleRate || 16000,
          channel_count: 1
        },
        recognition_config: {
          model: config.sttModel || 'nova-2',
          language_code: config.language || 'en-US',
          interim_results: true
        },
        synthesis_config: {
          voice_id: config.voiceId || 'default',
          speaking_rate: config.speakingRate || 1.0
        }
      }
    });

    return call;
  }

  sendAudio(call: grpc.ClientDuplexStream<any, any>, audioData: Buffer): void {
    call.write({
      audio: {
        audio_content: audioData
      }
    });
  }

  sendEndOfSpeech(call: grpc.ClientDuplexStream<any, any>): void {
    call.write({
      control: {
        signal: 'END_OF_SPEECH'
      }
    });
  }

  close(): void {
    this.client.close();
  }
}

// Usage
async function main() {
  const client = new VoiceAIClient();

  const call = client.streamingConversation({
    sampleRate: 16000,
    language: 'en-US',
    voiceId: 'alloy'
  });

  call.on('data', (response: any) => {
    if (response.transcription) {
      console.log(`Transcript: ${response.transcription.transcript}`);
    } else if (response.llm_response) {
      process.stdout.write(response.llm_response.text);
    } else if (response.audio) {
      console.log(`[Audio: ${response.audio.audio_content.length} bytes]`);
    } else if (response.status) {
      console.log(`Status: ${response.status.state}`);
    }
  });

  call.on('error', (err: Error) => {
    console.error('Error:', err);
  });

  call.on('end', () => {
    console.log('Stream ended');
  });

  // Simulate sending audio
  const audioChunk = Buffer.alloc(3200); // 100ms at 16kHz
  for (let i = 0; i < 50; i++) {
    client.sendAudio(call, audioChunk);
    await new Promise(resolve => setTimeout(resolve, 100));
  }

  client.sendEndOfSpeech(call);
}

main();
```

## Streaming Patterns

### Pattern 1: Unary (Request-Response)

```
Client                    Server
   |                         |
   |  Single Request         |
   |------------------------>|
   |                         |
   |  Single Response        |
   |<------------------------|
```

Use for: Configuration, session management

### Pattern 2: Server Streaming

```
Client                    Server
   |                         |
   |  TTS Request            |
   |------------------------>|
   |                         |
   |  Audio Chunk 1          |
   |<------------------------|
   |  Audio Chunk 2          |
   |<------------------------|
   |  Audio Chunk N          |
   |<------------------------|
```

Use for: TTS synthesis, long responses

### Pattern 3: Client Streaming

```
Client                    Server
   |                         |
   |  Audio Chunk 1          |
   |------------------------>|
   |  Audio Chunk 2          |
   |------------------------>|
   |  Audio Chunk N          |
   |------------------------>|
   |                         |
   |  Final Transcription    |
   |<------------------------|
```

Use for: Batch STT, file transcription

### Pattern 4: Bidirectional Streaming

```
Client                    Server
   |                         |
   |  Config                 |
   |------------------------>|
   |                    Ready|
   |<------------------------|
   |  Audio                  |
   |------------------------>|
   |              Interim    |
   |<------------------------|
   |  Audio                  |
   |------------------------>|
   |              Interim    |
   |<------------------------|
   |  End of Speech          |
   |------------------------>|
   |              Final      |
   |<------------------------|
   |              LLM Chunk  |
   |<------------------------|
   |              TTS Audio  |
   |<------------------------|
```

Use for: Real-time conversation (most voice AI applications)

## Flow Control and Backpressure

### Server-Side Flow Control

```python
class FlowControlledServicer(voice_ai_pb2_grpc.VoiceAIServiceServicer):
    def __init__(self):
        self.max_pending_audio_bytes = 1024 * 1024  # 1MB
        self.pending_audio = {}

    async def StreamingConversation(
        self,
        request_iterator,
        context
    ):
        session_id = context.peer()
        self.pending_audio[session_id] = 0

        async def process_with_backpressure():
            async for request in request_iterator:
                if request.HasField('audio'):
                    chunk_size = len(request.audio.audio_content)
                    self.pending_audio[session_id] += chunk_size

                    # Apply backpressure if buffer too large
                    while self.pending_audio[session_id] > self.max_pending_audio_bytes:
                        await asyncio.sleep(0.01)

                    yield request

        async for response in self._process_stream(process_with_backpressure()):
            yield response
            if response.HasField('transcription') and response.transcription.is_final:
                # Clear processed audio from count
                self.pending_audio[session_id] = 0

        del self.pending_audio[session_id]
```

### Client-Side Flow Control

```python
class FlowControlledClient:
    def __init__(self, stub):
        self.stub = stub
        self.send_queue = asyncio.Queue(maxsize=100)
        self.can_send = asyncio.Event()
        self.can_send.set()

    async def send_audio(self, audio_data: bytes):
        await self.can_send.wait()
        await self.send_queue.put(audio_data)

    async def request_generator(self):
        while True:
            audio_data = await self.send_queue.get()
            yield voice_ai_pb2.AudioRequest(
                audio=voice_ai_pb2.AudioChunk(audio_content=audio_data)
            )

    async def handle_responses(self, response_stream):
        async for response in response_stream:
            if response.HasField('status'):
                if response.status.state == voice_ai_pb2.StatusUpdate.PROCESSING:
                    # Server is processing, slow down sending
                    self.can_send.clear()
                elif response.status.state == voice_ai_pb2.StatusUpdate.LISTENING:
                    # Server ready for more audio
                    self.can_send.set()

            yield response
```

## Error Handling and Retries

### Retry Strategy

```python
import grpc
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)

class RetryableVoiceClient:
    RETRYABLE_CODES = {
        grpc.StatusCode.UNAVAILABLE,
        grpc.StatusCode.DEADLINE_EXCEEDED,
        grpc.StatusCode.RESOURCE_EXHAUSTED,
        grpc.StatusCode.ABORTED,
    }

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type(grpc.RpcError),
        before_sleep=lambda retry_state: print(f"Retrying... attempt {retry_state.attempt_number}")
    )
    async def streaming_conversation_with_retry(self, audio_stream, config):
        try:
            async for response in self.streaming_conversation(audio_stream, config):
                yield response
        except grpc.RpcError as e:
            if e.code() in self.RETRYABLE_CODES:
                raise  # Will be retried
            else:
                # Non-retryable error
                raise RuntimeError(f"Non-retryable gRPC error: {e.code()}")
```

### Graceful Degradation

```python
class ResilientVoiceClient:
    def __init__(self, primary_host, fallback_hosts):
        self.hosts = [primary_host] + fallback_hosts
        self.current_host_index = 0
        self.channel = None
        self.stub = None
        self._connect()

    def _connect(self):
        host = self.hosts[self.current_host_index]
        self.channel = grpc.aio.insecure_channel(host)
        self.stub = voice_ai_pb2_grpc.VoiceAIServiceStub(self.channel)

    async def _failover(self):
        self.current_host_index = (self.current_host_index + 1) % len(self.hosts)
        if self.channel:
            await self.channel.close()
        self._connect()

    async def streaming_conversation(self, audio_stream, config):
        attempts = 0
        max_attempts = len(self.hosts) * 2

        while attempts < max_attempts:
            try:
                async for response in self._do_streaming(audio_stream, config):
                    yield response
                return
            except grpc.RpcError as e:
                if e.code() == grpc.StatusCode.UNAVAILABLE:
                    await self._failover()
                    attempts += 1
                else:
                    raise

        raise RuntimeError("All hosts exhausted")
```

## Performance Optimization

### Connection Pooling

```python
import grpc
from typing import List

class GRPCConnectionPool:
    def __init__(self, host: str, pool_size: int = 10):
        self.channels: List[grpc.aio.Channel] = []
        self.stubs: List[voice_ai_pb2_grpc.VoiceAIServiceStub] = []
        self.current_index = 0

        for _ in range(pool_size):
            channel = grpc.aio.insecure_channel(
                host,
                options=[
                    ('grpc.max_concurrent_streams', 100),
                    ('grpc.keepalive_time_ms', 30000),
                ]
            )
            self.channels.append(channel)
            self.stubs.append(voice_ai_pb2_grpc.VoiceAIServiceStub(channel))

    def get_stub(self) -> voice_ai_pb2_grpc.VoiceAIServiceStub:
        stub = self.stubs[self.current_index]
        self.current_index = (self.current_index + 1) % len(self.stubs)
        return stub

    async def close(self):
        for channel in self.channels:
            await channel.close()
```

### Compression

```python
# Enable compression for large messages
channel = grpc.aio.insecure_channel(
    'localhost:50051',
    compression=grpc.Compression.Gzip,
    options=[
        ('grpc.default_compression_algorithm', grpc.Compression.Gzip),
        ('grpc.default_compression_level', grpc.CompressionLevel.Medium),
    ]
)
```

## Comparison: gRPC vs WebSocket

| Aspect | gRPC | WebSocket |
|--------|------|-----------|
| **Setup Complexity** | Higher (protobuf) | Lower |
| **Type Safety** | Excellent | Manual |
| **Performance** | Better | Good |
| **Browser Support** | grpc-web required | Native |
| **Debugging** | Specialized tools | Standard tools |
| **Ecosystem** | Growing | Mature |
| **Load Balancing** | L7 aware | Connection-level |

### When to Choose gRPC

- Backend-to-backend communication
- Type safety is critical
- Maximum performance needed
- Already using Protocol Buffers
- Complex streaming patterns

### When to Choose WebSocket

- Browser clients (without proxy)
- Simpler deployment
- Existing WebSocket infrastructure
- Rapid prototyping

## Related Topics

- **[WebSocket Protocols](/topics/infrastructure/streaming/websocket-protocols)** - Alternative streaming protocol
- **[Buffer Management](/topics/infrastructure/streaming/buffer-management)** - Audio buffer strategies
- **[Latency Optimization](/topics/foundations/latency-optimization)** - Minimizing end-to-end delay
- **[Audio Pipeline](/topics/infrastructure/audio-pipeline)** - Complete audio processing chain
