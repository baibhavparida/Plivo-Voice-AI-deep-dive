---
title: "WebRTC for Voice AI"
description: "Complete guide to WebRTC architecture, peer connections, ICE/STUN/TURN, audio codecs, and building browser-based voice agents"
category: "infrastructure"
tags: ["webrtc", "voice-ai", "browser", "real-time", "peer-connection", "ice", "stun", "turn", "opus", "audio-streaming"]
relatedTopics: ["sip-protocol", "audio-streaming", "latency-optimization", "signal-processing"]
---

# WebRTC for Voice AI

WebRTC (Web Real-Time Communication) is an open-source project that enables real-time audio, video, and data communication directly between browsers and applications without requiring plugins or intermediary servers for media transport. For voice AI systems, WebRTC provides the lowest-latency path for browser-based voice agents and offers built-in audio processing capabilities essential for high-quality voice interactions.

## WebRTC Architecture Overview

WebRTC establishes peer-to-peer connections for media transport while using signaling servers for connection setup and coordination.

```
+-----------------------------------------------------------------------------+
|                       WEBRTC ARCHITECTURE FOR VOICE AI                       |
+-----------------------------------------------------------------------------+
|                                                                              |
|  Browser/Client                                       Voice AI Server        |
|  +------------------+                              +------------------+      |
|  |                  |                              |                  |      |
|  | +------------+   |       Signaling              | +------------+   |      |
|  | |   User     |   |       (WebSocket/HTTP)       | |   Voice    |   |      |
|  | | Interface  |   |<---------------------------->| |   Agent    |   |      |
|  | +------------+   |       (SDP, ICE candidates)  | | Controller |   |      |
|  |       |          |                              | +------------+   |      |
|  |       v          |                              |       |          |      |
|  | +------------+   |                              |       v          |      |
|  | |    Web     |   |                              | +------------+   |      |
|  | |   Audio    |   |                              | |   WebRTC   |   |      |
|  | |    API     |   |                              | |  Endpoint  |   |      |
|  | +------------+   |                              | +------------+   |      |
|  |       |          |                              |       |          |      |
|  |       v          |       Media (RTP/SRTP)       |       v          |      |
|  | +------------+   |<=============================>| +------------+ |      |
|  | |   Peer     |   |       (Audio Streams)        | |    Peer    |   |      |
|  | | Connection |   |                              | | Connection |   |      |
|  | +------------+   |                              | +------------+   |      |
|  |                  |                              |       |          |      |
|  +------------------+                              |       v          |      |
|                                                    | +------------+   |      |
|                                                    | | STT / LLM  |   |      |
|                                                    | |   / TTS    |   |      |
|                                                    | +------------+   |      |
|                                                    +------------------+      |
|                                                                              |
+-----------------------------------------------------------------------------+
```

### WebRTC Components

| Component | Purpose | Voice AI Role |
|-----------|---------|---------------|
| PeerConnection | Manages connection state and media | Core API for audio streaming |
| MediaStream | Represents audio/video tracks | Captures user microphone input |
| RTCRtpSender | Sends media to remote peer | Transmits user speech to server |
| RTCRtpReceiver | Receives media from remote peer | Receives agent audio response |
| DataChannel | Arbitrary data transfer | Metadata, transcripts, events |
| ICE Agent | Connectivity establishment | NAT traversal and path finding |

## Peer Connection Establishment

The WebRTC connection process involves offer/answer negotiation using Session Description Protocol (SDP).

### Connection Flow

```
+----------+              +----------+              +----------+
|  Browser |              | Signaling|              | Voice AI |
|  Client  |              |  Server  |              |  Server  |
+----+-----+              +----+-----+              +----+-----+
     |                         |                         |
     | 1. Create PeerConnection|                         |
     |------------------------>|                         |
     |                         |                         |
     | 2. Add audio track      |                         |
     |    (getUserMedia)       |                         |
     |------------------------>|                         |
     |                         |                         |
     | 3. Create Offer (SDP)   |                         |
     |------------------------>|                         |
     |                         | 4. Forward Offer        |
     |                         |------------------------>|
     |                         |                         |
     |                         |                         | 5. Create Answer
     |                         |                         |    (SDP)
     |                         | 6. Send Answer          |
     |                         |<------------------------|
     | 7. Set Remote Desc      |                         |
     |<------------------------|                         |
     |                         |                         |
     | 8. ICE Candidate        |                         |
     |------------------------>| 9. Forward Candidate    |
     |                         |------------------------>|
     |                         |                         |
     |                         | 10. ICE Candidate       |
     |<------------------------|<------------------------|
     |                         |                         |
     |                         |                         |
     |<=================== Media Flow =================>|
     |                         |                         |
```

### JavaScript Implementation

```javascript
class VoiceAIWebRTCClient {
    constructor(signalingUrl) {
        this.signalingUrl = signalingUrl;
        this.peerConnection = null;
        this.localStream = null;
        this.ws = null;
    }

    async initialize() {
        // Connect to signaling server
        this.ws = new WebSocket(this.signalingUrl);
        this.ws.onmessage = (event) => this.handleSignalingMessage(JSON.parse(event.data));

        // Configure peer connection with ICE servers
        const config = {
            iceServers: [
                { urls: 'stun:stun.l.google.com:19302' },
                {
                    urls: 'turn:turn.example.com:3478',
                    username: 'user',
                    credential: 'password'
                }
            ],
            iceCandidatePoolSize: 10
        };

        this.peerConnection = new RTCPeerConnection(config);

        // Set up event handlers
        this.peerConnection.onicecandidate = (event) => {
            if (event.candidate) {
                this.sendSignalingMessage({
                    type: 'ice-candidate',
                    candidate: event.candidate
                });
            }
        };

        this.peerConnection.ontrack = (event) => {
            // Handle incoming audio from voice agent
            this.handleRemoteAudio(event.streams[0]);
        };

        this.peerConnection.onconnectionstatechange = () => {
            console.log('Connection state:', this.peerConnection.connectionState);
            this.onConnectionStateChange(this.peerConnection.connectionState);
        };

        // Get user's microphone
        await this.setupLocalAudio();
    }

    async setupLocalAudio() {
        const constraints = {
            audio: {
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true,
                sampleRate: 16000,
                channelCount: 1
            },
            video: false
        };

        try {
            this.localStream = await navigator.mediaDevices.getUserMedia(constraints);

            // Add audio track to peer connection
            this.localStream.getAudioTracks().forEach(track => {
                this.peerConnection.addTrack(track, this.localStream);
            });

            console.log('Local audio initialized');
        } catch (error) {
            console.error('Failed to access microphone:', error);
            throw error;
        }
    }

    async startCall() {
        // Create and send offer
        const offer = await this.peerConnection.createOffer({
            offerToReceiveAudio: true,
            offerToReceiveVideo: false
        });

        await this.peerConnection.setLocalDescription(offer);

        this.sendSignalingMessage({
            type: 'offer',
            sdp: offer.sdp
        });
    }

    async handleSignalingMessage(message) {
        switch (message.type) {
            case 'answer':
                const answer = new RTCSessionDescription({
                    type: 'answer',
                    sdp: message.sdp
                });
                await this.peerConnection.setRemoteDescription(answer);
                break;

            case 'ice-candidate':
                if (message.candidate) {
                    await this.peerConnection.addIceCandidate(
                        new RTCIceCandidate(message.candidate)
                    );
                }
                break;

            case 'transcript':
                this.onTranscript(message.text, message.isFinal);
                break;

            case 'agent-speaking':
                this.onAgentSpeaking(message.text);
                break;
        }
    }

    handleRemoteAudio(stream) {
        // Create audio element for playback
        const audioElement = document.getElementById('agent-audio') || document.createElement('audio');
        audioElement.id = 'agent-audio';
        audioElement.srcObject = stream;
        audioElement.autoplay = true;
        document.body.appendChild(audioElement);
    }

    sendSignalingMessage(message) {
        this.ws.send(JSON.stringify(message));
    }

    disconnect() {
        if (this.localStream) {
            this.localStream.getTracks().forEach(track => track.stop());
        }
        if (this.peerConnection) {
            this.peerConnection.close();
        }
        if (this.ws) {
            this.ws.close();
        }
    }

    // Event callbacks (override in application)
    onConnectionStateChange(state) {}
    onTranscript(text, isFinal) {}
    onAgentSpeaking(text) {}
}
```

### Server-Side WebRTC Handler (Python)

```python
import asyncio
from aiortc import RTCPeerConnection, RTCSessionDescription, MediaStreamTrack
from aiortc.contrib.media import MediaPlayer, MediaRecorder

class VoiceAIWebRTCServer:
    """WebRTC server for voice AI applications"""

    def __init__(self, voice_agent):
        self.voice_agent = voice_agent
        self.peer_connections = {}

    async def handle_offer(self, session_id: str, offer_sdp: str) -> dict:
        """Process WebRTC offer and create answer"""
        pc = RTCPeerConnection()
        self.peer_connections[session_id] = pc

        # Set up event handlers
        @pc.on("track")
        async def on_track(track):
            if track.kind == "audio":
                # Process incoming audio from user
                asyncio.create_task(
                    self.process_user_audio(session_id, track)
                )

        @pc.on("connectionstatechange")
        async def on_connection_state_change():
            if pc.connectionState == "failed":
                await self.cleanup_session(session_id)

        # Add outgoing audio track for agent responses
        agent_audio_track = AgentAudioTrack(self.voice_agent)
        pc.addTrack(agent_audio_track)
        self.voice_agent.set_audio_output(agent_audio_track)

        # Set remote description (offer) and create answer
        offer = RTCSessionDescription(sdp=offer_sdp, type="offer")
        await pc.setRemoteDescription(offer)

        answer = await pc.createAnswer()
        await pc.setLocalDescription(answer)

        return {
            "type": "answer",
            "sdp": pc.localDescription.sdp
        }

    async def add_ice_candidate(self, session_id: str, candidate: dict):
        """Add ICE candidate from remote peer"""
        pc = self.peer_connections.get(session_id)
        if pc:
            await pc.addIceCandidate(candidate)

    async def process_user_audio(self, session_id: str, track: MediaStreamTrack):
        """Process incoming audio and send to voice agent"""
        while True:
            try:
                frame = await track.recv()

                # Convert audio frame to bytes
                audio_data = frame.to_ndarray().tobytes()

                # Send to voice agent for processing
                await self.voice_agent.process_audio(session_id, audio_data)

            except Exception as e:
                print(f"Audio processing error: {e}")
                break

    async def cleanup_session(self, session_id: str):
        """Clean up resources for a session"""
        pc = self.peer_connections.pop(session_id, None)
        if pc:
            await pc.close()


class AgentAudioTrack(MediaStreamTrack):
    """Custom audio track for sending agent responses"""

    kind = "audio"

    def __init__(self, voice_agent):
        super().__init__()
        self.voice_agent = voice_agent
        self.audio_queue = asyncio.Queue()

    async def recv(self):
        """Get next audio frame to send"""
        frame = await self.audio_queue.get()
        return frame

    def push_audio(self, audio_frame):
        """Push audio frame to be sent"""
        try:
            self.audio_queue.put_nowait(audio_frame)
        except asyncio.QueueFull:
            # Drop frame if queue is full
            pass
```

## ICE, STUN, and TURN

Interactive Connectivity Establishment (ICE) is a framework for NAT traversal that uses STUN and TURN servers to establish connectivity.

### ICE Candidate Types

```
+-----------------------------------------------------------------------------+
|                        ICE CANDIDATE GATHERING                               |
+-----------------------------------------------------------------------------+
|                                                                              |
|  Candidate Type      Description                    Priority                 |
|  +-----------+       +------------------------+     +--------+              |
|  |   Host    | ----> | Local IP address       | --> | Highest|              |
|  +-----------+       +------------------------+     +--------+              |
|        |                                                                     |
|        v                                                                     |
|  +-----------+       +------------------------+     +--------+              |
|  |   srflx   | ----> | Server Reflexive       | --> | Medium |              |
|  | (STUN)    |       | (Public IP via STUN)   |     |        |              |
|  +-----------+       +------------------------+     +--------+              |
|        |                                                                     |
|        v                                                                     |
|  +-----------+       +------------------------+     +--------+              |
|  |   relay   | ----> | Relayed via TURN       | --> | Lowest |              |
|  | (TURN)    |       | (Guaranteed connection)|     |        |              |
|  +-----------+       +------------------------+     +--------+              |
|                                                                              |
|  ICE prioritizes direct connections but falls back to relay when needed     |
|                                                                              |
+-----------------------------------------------------------------------------+
```

### STUN (Session Traversal Utilities for NAT)

STUN allows a client to discover its public IP address and the type of NAT it's behind.

```
+----------+                    +----------+                    +----------+
|  Client  |                    |   NAT    |                    |   STUN   |
|192.168.1.x|                   |          |                    |  Server  |
+----+-----+                    +----+-----+                    +----+-----+
     |                               |                               |
     | Binding Request               |                               |
     |------------------------------>|                               |
     |                               | Binding Request               |
     |                               |   (from 203.0.113.1:12345)    |
     |                               |------------------------------>|
     |                               |                               |
     |                               |              Binding Response |
     |                               |     (XOR-MAPPED: 203.0.113.1) |
     |                               |<------------------------------|
     |          Binding Response     |                               |
     |     (XOR-MAPPED: 203.0.113.1) |                               |
     |<------------------------------|                               |
     |                               |                               |
     | Now knows public IP: 203.0.113.1:12345                        |
     |                                                               |
```

### TURN (Traversal Using Relays around NAT)

TURN provides a relay server when direct connectivity fails (symmetric NAT, firewall).

```
+-----------------------------------------------------------------------------+
|                        TURN RELAY ARCHITECTURE                               |
+-----------------------------------------------------------------------------+
|                                                                              |
|  Client A                      TURN Server                      Client B    |
|  (Voice AI)                                                    (Browser)    |
|  +--------+                    +----------+                    +--------+   |
|  |        |   Allocate Req     |          |                    |        |   |
|  |        |------------------->|          |                    |        |   |
|  |        |                    |          |                    |        |   |
|  |        |   Allocate Resp    |          |                    |        |   |
|  |        |   (Relay Addr)     |          |                    |        |   |
|  |        |<-------------------|          |                    |        |   |
|  |        |                    |          |                    |        |   |
|  |        |   Send via relay   |          |   Forward to B     |        |   |
|  |        |===================>|          |===================>|        |   |
|  |        |                    |          |                    |        |   |
|  |        |   Forward from B   |          |   Send via relay   |        |   |
|  |        |<===================|          |<===================|        |   |
|  +--------+                    +----------+                    +--------+   |
|                                                                              |
+-----------------------------------------------------------------------------+
```

### ICE Configuration for Voice AI

```javascript
const iceConfig = {
    iceServers: [
        // Google's public STUN server
        { urls: 'stun:stun.l.google.com:19302' },
        { urls: 'stun:stun1.l.google.com:19302' },

        // Self-hosted TURN server (recommended for production)
        {
            urls: [
                'turn:turn.example.com:3478?transport=udp',
                'turn:turn.example.com:3478?transport=tcp',
                'turns:turn.example.com:5349?transport=tcp'
            ],
            username: 'voice-ai-user',
            credential: 'secure-credential'
        },

        // Commercial TURN service (Twilio, Xirsys)
        {
            urls: 'turn:global.turn.twilio.com:3478?transport=udp',
            username: 'twilio-username',
            credential: 'twilio-credential'
        }
    ],

    // ICE transport policy
    // 'all' - Use both relay and direct candidates
    // 'relay' - Force TURN relay only (for privacy/firewall)
    iceTransportPolicy: 'all',

    // Bundle policy for multiplexing
    bundlePolicy: 'max-bundle',

    // RTCP multiplexing
    rtcpMuxPolicy: 'require',

    // ICE candidate pool size (pre-gather candidates)
    iceCandidatePoolSize: 10
};
```

## Audio Codecs

WebRTC supports multiple audio codecs, with Opus being the preferred choice for voice AI applications.

### Codec Comparison

| Codec | Sample Rates | Bitrate | Latency | Quality | Use Case |
|-------|--------------|---------|---------|---------|----------|
| Opus | 8-48 kHz | 6-510 kbps | 2.5-60ms | Excellent | Voice AI (recommended) |
| G.711 PCMU | 8 kHz | 64 kbps | 0.125ms | Good | Legacy/PSTN |
| G.711 PCMA | 8 kHz | 64 kbps | 0.125ms | Good | Legacy/PSTN |
| G.722 | 16 kHz | 48-64 kbps | 4ms | Very Good | Wideband voice |
| iSAC | 16-32 kHz | 10-52 kbps | 30-60ms | Good | Variable bandwidth |
| iLBC | 8 kHz | 13.3-15.2 kbps | 25-40ms | Fair | Packet loss resilient |

### Opus Configuration for Voice AI

```javascript
// SDP manipulation to prioritize Opus
function prioritizeOpus(sdp) {
    const lines = sdp.split('\r\n');
    const audioLineIndex = lines.findIndex(line => line.startsWith('m=audio'));

    if (audioLineIndex === -1) return sdp;

    const audioLine = lines[audioLineIndex];
    const parts = audioLine.split(' ');

    // Find Opus payload type (usually 111)
    const opusPayload = lines
        .filter(line => line.includes('opus/48000'))
        .map(line => line.match(/a=rtpmap:(\d+)/)?.[1])
        .find(Boolean);

    if (opusPayload) {
        // Move Opus to first position in payload list
        const payloadTypes = parts.slice(3);
        const reordered = [opusPayload, ...payloadTypes.filter(p => p !== opusPayload)];
        parts.splice(3, payloadTypes.length, ...reordered);
        lines[audioLineIndex] = parts.join(' ');
    }

    return lines.join('\r\n');
}

// Configure Opus parameters via SDP
function configureOpusForVoiceAI(sdp) {
    // Optimal Opus settings for voice AI
    const opusParams = [
        'maxaveragebitrate=32000',    // 32 kbps average
        'maxplaybackrate=16000',       // 16 kHz playback (voice range)
        'sprop-maxcapturerate=16000',  // 16 kHz capture
        'stereo=0',                    // Mono audio
        'useinbandfec=1',              // Enable forward error correction
        'usedtx=0'                     // Disable discontinuous transmission
    ].join(';');

    return sdp.replace(
        /a=fmtp:111 .*/,
        `a=fmtp:111 ${opusParams}`
    );
}
```

### Codec Selection Strategy

```python
class CodecSelector:
    """Select optimal codec based on network conditions"""

    def __init__(self):
        self.codec_preferences = [
            {
                "name": "opus",
                "rate": 48000,
                "channels": 2,
                "parameters": {
                    "maxaveragebitrate": 32000,
                    "maxplaybackrate": 16000,
                    "useinbandfec": 1
                },
                "min_bandwidth": 20000,  # 20 kbps minimum
                "packet_loss_tolerance": 15  # Up to 15% loss
            },
            {
                "name": "G722",
                "rate": 8000,
                "channels": 1,
                "parameters": {},
                "min_bandwidth": 64000,
                "packet_loss_tolerance": 5
            },
            {
                "name": "PCMU",
                "rate": 8000,
                "channels": 1,
                "parameters": {},
                "min_bandwidth": 64000,
                "packet_loss_tolerance": 2
            }
        ]

    def select_codec(self, bandwidth_kbps: int, packet_loss_percent: float) -> dict:
        """Select best codec for current conditions"""
        for codec in self.codec_preferences:
            if (bandwidth_kbps >= codec["min_bandwidth"] / 1000 and
                packet_loss_percent <= codec["packet_loss_tolerance"]):
                return codec

        # Fallback to most resilient codec
        return self.codec_preferences[0]  # Opus with FEC
```

## Media Streams and Tracks

### MediaStream API for Voice AI

```javascript
class VoiceAIMediaHandler {
    constructor() {
        this.localStream = null;
        this.audioContext = null;
        this.analyser = null;
        this.processors = [];
    }

    async initializeAudio() {
        // Get microphone with optimized constraints
        this.localStream = await navigator.mediaDevices.getUserMedia({
            audio: {
                // Echo cancellation (critical for voice AI)
                echoCancellation: { ideal: true },
                // Noise suppression
                noiseSuppression: { ideal: true },
                // Auto gain control
                autoGainControl: { ideal: true },
                // Sample rate (16kHz optimal for speech)
                sampleRate: { ideal: 16000 },
                // Mono audio
                channelCount: { ideal: 1 },
                // Latency hints
                latency: { ideal: 0.01 },  // 10ms target
            }
        });

        // Set up audio analysis
        this.audioContext = new AudioContext({ sampleRate: 16000 });
        const source = this.audioContext.createMediaStreamSource(this.localStream);

        // Analyser for volume metering
        this.analyser = this.audioContext.createAnalyser();
        this.analyser.fftSize = 256;
        source.connect(this.analyser);

        return this.localStream;
    }

    getAudioLevel() {
        if (!this.analyser) return 0;

        const dataArray = new Uint8Array(this.analyser.frequencyBinCount);
        this.analyser.getByteFrequencyData(dataArray);

        // Calculate RMS
        let sum = 0;
        for (let i = 0; i < dataArray.length; i++) {
            sum += dataArray[i] * dataArray[i];
        }
        return Math.sqrt(sum / dataArray.length);
    }

    async applyAudioProcessing(stream) {
        const audioContext = new AudioContext();
        const source = audioContext.createMediaStreamSource(stream);

        // Create processing chain
        const highpassFilter = audioContext.createBiquadFilter();
        highpassFilter.type = 'highpass';
        highpassFilter.frequency.value = 80;  // Remove sub-80Hz rumble

        const lowpassFilter = audioContext.createBiquadFilter();
        lowpassFilter.type = 'lowpass';
        lowpassFilter.frequency.value = 8000;  // Focus on speech frequencies

        const compressor = audioContext.createDynamicsCompressor();
        compressor.threshold.value = -24;
        compressor.knee.value = 30;
        compressor.ratio.value = 12;
        compressor.attack.value = 0.003;
        compressor.release.value = 0.25;

        // Connect chain
        source
            .connect(highpassFilter)
            .connect(lowpassFilter)
            .connect(compressor);

        // Create output stream
        const destination = audioContext.createMediaStreamDestination();
        compressor.connect(destination);

        return destination.stream;
    }

    setTrackEnabled(enabled) {
        if (this.localStream) {
            this.localStream.getAudioTracks().forEach(track => {
                track.enabled = enabled;
            });
        }
    }

    replaceTrack(peerConnection, newTrack) {
        const sender = peerConnection.getSenders()
            .find(s => s.track?.kind === 'audio');

        if (sender) {
            return sender.replaceTrack(newTrack);
        }
    }
}
```

### Track Statistics and Monitoring

```javascript
class WebRTCStatsMonitor {
    constructor(peerConnection) {
        this.pc = peerConnection;
        this.statsHistory = [];
    }

    async getAudioStats() {
        const stats = await this.pc.getStats();
        const audioStats = {
            inbound: null,
            outbound: null,
            timestamp: Date.now()
        };

        stats.forEach(report => {
            if (report.type === 'inbound-rtp' && report.kind === 'audio') {
                audioStats.inbound = {
                    packetsReceived: report.packetsReceived,
                    packetsLost: report.packetsLost,
                    jitter: report.jitter,
                    bytesReceived: report.bytesReceived,
                    framesDecoded: report.framesDecoded,
                    concealedSamples: report.concealedSamples,  // Packet loss concealment
                    totalSamplesReceived: report.totalSamplesReceived
                };
            }

            if (report.type === 'outbound-rtp' && report.kind === 'audio') {
                audioStats.outbound = {
                    packetsSent: report.packetsSent,
                    bytesSent: report.bytesSent,
                    targetBitrate: report.targetBitrate,
                    retransmittedPacketsSent: report.retransmittedPacketsSent
                };
            }

            if (report.type === 'candidate-pair' && report.state === 'succeeded') {
                audioStats.connection = {
                    roundTripTime: report.currentRoundTripTime * 1000,  // ms
                    availableOutgoingBitrate: report.availableOutgoingBitrate,
                    bytesReceived: report.bytesReceived,
                    bytesSent: report.bytesSent
                };
            }
        });

        this.statsHistory.push(audioStats);
        return audioStats;
    }

    calculateQualityMetrics(stats) {
        if (!stats.inbound) return null;

        const totalPackets = stats.inbound.packetsReceived + stats.inbound.packetsLost;
        const packetLossRate = totalPackets > 0
            ? (stats.inbound.packetsLost / totalPackets) * 100
            : 0;

        const concealmentRate = stats.inbound.totalSamplesReceived > 0
            ? (stats.inbound.concealedSamples / stats.inbound.totalSamplesReceived) * 100
            : 0;

        return {
            packetLossPercent: packetLossRate.toFixed(2),
            jitterMs: (stats.inbound.jitter * 1000).toFixed(2),
            rttMs: stats.connection?.roundTripTime?.toFixed(2) || 'N/A',
            concealmentPercent: concealmentRate.toFixed(2),
            quality: this.assessQuality(packetLossRate, stats.inbound.jitter * 1000)
        };
    }

    assessQuality(packetLoss, jitter) {
        if (packetLoss < 1 && jitter < 20) return 'Excellent';
        if (packetLoss < 3 && jitter < 50) return 'Good';
        if (packetLoss < 5 && jitter < 100) return 'Fair';
        return 'Poor';
    }
}
```

## WebRTC vs WebSocket for Audio

### Comparison for Voice AI

| Aspect | WebRTC | WebSocket |
|--------|--------|-----------|
| Protocol | UDP preferred (DTLS/SRTP) | TCP (WebSocket over HTTP) |
| Latency | Lower (no HoL blocking) | Higher (TCP retransmission) |
| Packet Loss | Concealment (PLC) | Guaranteed delivery |
| Built-in AEC | Yes | No |
| Built-in AGC | Yes | No |
| Built-in Noise Suppression | Yes | No |
| NAT Traversal | ICE/STUN/TURN | Simple proxy |
| Implementation Complexity | Higher | Lower |
| Browser Support | Modern browsers | Universal |
| Server Scaling | More complex | Simpler |

### Decision Framework

```
+-----------------------------------------------------------------------------+
|                  WEBRTC VS WEBSOCKET DECISION TREE                           |
+-----------------------------------------------------------------------------+
|                                                                              |
|  Start                                                                       |
|    |                                                                         |
|    v                                                                         |
|  Need ultra-low latency (under 200ms RTT)?                                       |
|    |                                                                         |
|    +-- Yes --> Need browser AEC/AGC/NS?                                     |
|    |              |                                                          |
|    |              +-- Yes --> Use WebRTC                                     |
|    |              |                                                          |
|    |              +-- No --> Consider both                                   |
|    |                                                                         |
|    +-- No --> Simple server-side processing?                                |
|                |                                                             |
|                +-- Yes --> Use WebSocket                                     |
|                |                                                             |
|                +-- No --> Running in browser?                                |
|                             |                                                |
|                             +-- Yes --> Use WebRTC                           |
|                             |                                                |
|                             +-- No --> Use WebSocket                         |
|                                                                              |
+-----------------------------------------------------------------------------+
```

### Hybrid Approach

```javascript
class HybridVoiceConnection {
    /**
     * Use WebRTC for media and WebSocket for signaling/data
     */

    constructor(config) {
        this.webrtcClient = null;
        this.websocketClient = null;
        this.config = config;
    }

    async connect() {
        // WebSocket for signaling and real-time data
        this.websocketClient = new WebSocket(this.config.signalingUrl);

        this.websocketClient.onmessage = async (event) => {
            const message = JSON.parse(event.data);
            await this.handleSignalingMessage(message);
        };

        await this.waitForConnection(this.websocketClient);

        // WebRTC for media
        await this.initializeWebRTC();
    }

    async initializeWebRTC() {
        this.webrtcClient = new RTCPeerConnection(this.config.iceServers);

        // Get audio with browser processing
        const stream = await navigator.mediaDevices.getUserMedia({
            audio: {
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true
            }
        });

        stream.getTracks().forEach(track => {
            this.webrtcClient.addTrack(track, stream);
        });

        // Create data channel for low-latency events
        this.dataChannel = this.webrtcClient.createDataChannel('events', {
            ordered: false,  // Allow out-of-order delivery
            maxRetransmits: 0  // No retransmission (UDP-like)
        });

        this.dataChannel.onmessage = (event) => {
            this.handleDataChannelMessage(JSON.parse(event.data));
        };

        // Start offer/answer exchange
        const offer = await this.webrtcClient.createOffer();
        await this.webrtcClient.setLocalDescription(offer);

        this.websocketClient.send(JSON.stringify({
            type: 'offer',
            sdp: offer.sdp
        }));
    }

    // Send time-critical data via DataChannel
    sendEvent(event) {
        if (this.dataChannel?.readyState === 'open') {
            this.dataChannel.send(JSON.stringify(event));
        }
    }

    // Send non-critical data via WebSocket
    sendMessage(message) {
        if (this.websocketClient?.readyState === WebSocket.OPEN) {
            this.websocketClient.send(JSON.stringify(message));
        }
    }
}
```

## Browser-Based Voice Agents

### Complete Voice AI Client Implementation

```javascript
class BrowserVoiceAgent {
    constructor(options) {
        this.options = {
            signalingUrl: options.signalingUrl,
            iceServers: options.iceServers,
            onTranscript: options.onTranscript || (() => {}),
            onAgentResponse: options.onAgentResponse || (() => {}),
            onStateChange: options.onStateChange || (() => {}),
            ...options
        };

        this.state = 'disconnected';
        this.peerConnection = null;
        this.dataChannel = null;
        this.localStream = null;
        this.ws = null;
    }

    async start() {
        this.setState('connecting');

        try {
            // Initialize WebSocket signaling
            await this.initializeSignaling();

            // Initialize WebRTC
            await this.initializeWebRTC();

            // Start the call
            await this.createOffer();

            this.setState('connected');
        } catch (error) {
            this.setState('error');
            throw error;
        }
    }

    async initializeSignaling() {
        return new Promise((resolve, reject) => {
            this.ws = new WebSocket(this.options.signalingUrl);

            this.ws.onopen = () => resolve();
            this.ws.onerror = (error) => reject(error);

            this.ws.onmessage = async (event) => {
                const message = JSON.parse(event.data);
                await this.handleSignalingMessage(message);
            };

            this.ws.onclose = () => {
                if (this.state === 'connected') {
                    this.handleDisconnection();
                }
            };
        });
    }

    async initializeWebRTC() {
        this.peerConnection = new RTCPeerConnection({
            iceServers: this.options.iceServers
        });

        // ICE candidate handling
        this.peerConnection.onicecandidate = (event) => {
            if (event.candidate) {
                this.send({ type: 'ice-candidate', candidate: event.candidate });
            }
        };

        // Handle incoming audio from agent
        this.peerConnection.ontrack = (event) => {
            const audio = new Audio();
            audio.srcObject = event.streams[0];
            audio.play();
        };

        // Connection state monitoring
        this.peerConnection.onconnectionstatechange = () => {
            const state = this.peerConnection.connectionState;
            if (state === 'failed' || state === 'disconnected') {
                this.handleDisconnection();
            }
        };

        // Get microphone
        this.localStream = await navigator.mediaDevices.getUserMedia({
            audio: {
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true,
                sampleRate: 16000
            }
        });

        this.localStream.getTracks().forEach(track => {
            this.peerConnection.addTrack(track, this.localStream);
        });

        // Create data channel for transcripts and events
        this.dataChannel = this.peerConnection.createDataChannel('voiceai', {
            ordered: true
        });

        this.dataChannel.onmessage = (event) => {
            const data = JSON.parse(event.data);
            this.handleDataChannelMessage(data);
        };
    }

    async createOffer() {
        const offer = await this.peerConnection.createOffer({
            offerToReceiveAudio: true
        });

        await this.peerConnection.setLocalDescription(offer);
        this.send({ type: 'offer', sdp: offer.sdp });
    }

    async handleSignalingMessage(message) {
        switch (message.type) {
            case 'answer':
                await this.peerConnection.setRemoteDescription(
                    new RTCSessionDescription({ type: 'answer', sdp: message.sdp })
                );
                break;

            case 'ice-candidate':
                await this.peerConnection.addIceCandidate(
                    new RTCIceCandidate(message.candidate)
                );
                break;
        }
    }

    handleDataChannelMessage(data) {
        switch (data.type) {
            case 'transcript':
                this.options.onTranscript(data.text, data.isFinal);
                break;

            case 'agent_response':
                this.options.onAgentResponse(data.text);
                break;

            case 'state':
                this.setState(data.state);
                break;
        }
    }

    send(message) {
        if (this.ws?.readyState === WebSocket.OPEN) {
            this.ws.send(JSON.stringify(message));
        }
    }

    setState(state) {
        this.state = state;
        this.options.onStateChange(state);
    }

    mute() {
        this.localStream?.getAudioTracks().forEach(track => {
            track.enabled = false;
        });
    }

    unmute() {
        this.localStream?.getAudioTracks().forEach(track => {
            track.enabled = true;
        });
    }

    async stop() {
        this.setState('disconnecting');

        // Stop local media
        this.localStream?.getTracks().forEach(track => track.stop());

        // Close peer connection
        this.peerConnection?.close();

        // Close WebSocket
        this.ws?.close();

        this.setState('disconnected');
    }

    handleDisconnection() {
        this.setState('disconnected');

        // Attempt reconnection
        if (this.options.autoReconnect) {
            setTimeout(() => this.start(), 3000);
        }
    }
}

// Usage
const agent = new BrowserVoiceAgent({
    signalingUrl: 'wss://voice-ai.example.com/ws',
    iceServers: [
        { urls: 'stun:stun.l.google.com:19302' },
        { urls: 'turn:turn.example.com', username: 'user', credential: 'pass' }
    ],
    onTranscript: (text, isFinal) => {
        console.log(`Transcript${isFinal ? ' (final)' : ''}: ${text}`);
    },
    onAgentResponse: (text) => {
        console.log(`Agent: ${text}`);
    },
    onStateChange: (state) => {
        console.log(`State: ${state}`);
    },
    autoReconnect: true
});

await agent.start();
```

## Summary

WebRTC provides the foundation for browser-based voice AI with:

1. **Peer Connections**: Direct media paths for lowest latency
2. **ICE/STUN/TURN**: Reliable connectivity through NATs and firewalls
3. **Opus Codec**: Optimal audio quality with built-in error resilience
4. **Built-in Processing**: AEC, AGC, and noise suppression without extra code
5. **Media Streams**: Fine-grained control over audio capture and playback

For production voice AI, WebRTC offers superior real-time performance compared to WebSocket-based solutions, while the hybrid approach combines the best of both protocols for a complete solution.
